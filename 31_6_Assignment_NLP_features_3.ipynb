{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "31.6 Assignment NLP - features -3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFUUcJ2W5BlY5SHkquRMD2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexBB999/NLP/blob/master/31_6_Assignment_NLP_features_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYSpzK37z-dI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSWY7Ua10Qv9",
        "colab_type": "code",
        "outputId": "ea8883d0-b0c0-4f85-d846-79d1a7091223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "import gensim\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "!python -m spacy download en"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBg38oKY0dNO",
        "colab_type": "text"
      },
      "source": [
        "**UTITLITY FUNCTION FOR STANDARD TEXT CLEANING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLKQpf1c0l6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utility function for standard text cleaning\n",
        "def text_cleaner(text):\n",
        "    # visual inspection identifies a form of punctuation spaCy does not\n",
        "    # recognize: the double dash '--'.  Better get rid of it now!\n",
        "    text = re.sub(r'--',' ',text)\n",
        "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
        "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwsvpsoG0pY0",
        "colab_type": "text"
      },
      "source": [
        "**LOAD AND CLEAN THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHaZfNgs0tHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load and clean the data\n",
        "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
        "alice = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# the chapter indicator is idiosyncratic\n",
        "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
        "alice = re.sub(r'CHAPTER .*', '', alice)\n",
        "    \n",
        "alice = text_cleaner(alice)\n",
        "persuasion = text_cleaner(persuasion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vnxgCu90zCw",
        "colab_type": "text"
      },
      "source": [
        "**PARSE THE CLEANED DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3y5wNcq03vC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parse the cleaned novels. This can take a bit.\n",
        "nlp = spacy.load('en')\n",
        "alice_doc = nlp(alice)\n",
        "persuasion_doc = nlp(persuasion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHX9SOoC1Au0",
        "colab_type": "text"
      },
      "source": [
        "**GROUP INTO SENTENCES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDpyxBZp1HT_",
        "colab_type": "code",
        "outputId": "f19686bb-fe34-48b7-e7b8-cb30b506effb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# group into sentences\n",
        "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
        "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
        "\n",
        "# combine the sentences from the two novels into one data frame\n",
        "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
        "sentences.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(I, shall, be, late, !, ')</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text   author\n",
              "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
              "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
              "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
              "3                                      (Oh, dear, !)  Carroll\n",
              "4                         (I, shall, be, late, !, ')  Carroll"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgGORu-c1KjD",
        "colab_type": "text"
      },
      "source": [
        "**GET RID OF STOP WORDS AND PUNCTUATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJC3y6dW1P1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get rid off stop words and punctuation\n",
        "# and lemmatize the tokens\n",
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "    sentences.loc[i, \"text\"] = [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ueahp1cT1Wnx",
        "colab_type": "text"
      },
      "source": [
        "**NOW READY TO VECTORIZE USING WORD2VEC**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51kwVhsM1kTP",
        "colab_type": "text"
      },
      "source": [
        "**The Word2Vec class has several parameters**.\n",
        "\n",
        "We set the following parameters:\n",
        "\n",
        "**workers**=4: We set the number of threads to run in parallel to 4 (make sense if your computer has available computing units).\n",
        "\n",
        "**min_count**=1: We set the minimum word count threshold to 1.\n",
        "\n",
        "**window**=6: We set the number of words around target word to consider to 6.\n",
        "\n",
        "**sg**=0: We use CBOW because our corpus is small.\n",
        "\n",
        "**sample**=1e-3: We penalize frequent words.\n",
        "\n",
        "**size**=100: We set the word vector length to 100.\n",
        "\n",
        "**hs**=1: We use hierarchical softmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9aQ3ua4122w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train word2vec on the the sentences\n",
        "model = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=6,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=100,\n",
        "    hs=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsQ5U-Cl2FZH",
        "colab_type": "text"
      },
      "source": [
        "Before jumping in the machine learning model for prediction,\n",
        "\n",
        " **let's play with our word2vec word representation we just trained**. \n",
        " \n",
        " **specifically, we'll look into**:\n",
        "\n",
        "The first five words that are closer to lady.\n",
        "\n",
        "The word that doesn't fit in list: dad dinner mom aunt uncle.\n",
        "\n",
        "The similarity score of woman and man.\n",
        "\n",
        "The similarity score of horse and cat.\n",
        "\n",
        "Note that all of the above calculations are based on the word2vec representations of the words we just trained above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqhvb8vh2Sy5",
        "colab_type": "code",
        "outputId": "93954840-3b9a-4327-86bf-cfb4a298a906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(model.most_similar(positive=['lady', 'man'], negative=['woman'], topn=5))\n",
        "print(model.doesnt_match(\"dad dinner mom aunt uncle\".split()))\n",
        "print(model.similarity('woman', 'man'))\n",
        "print(model.similarity('horse', 'cat'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('heart', 0.9986150860786438), ('head', 0.9980312585830688), ('receive', 0.9979260563850403), ('send', 0.9978547096252441), ('compare', 0.9978461265563965)]\n",
            "dinner\n",
            "0.99793494\n",
            "0.91907066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VlIZZz82oSm",
        "colab_type": "text"
      },
      "source": [
        "Well, the results make sense to some degree but it's obvious that our representations aren't perfect.\n",
        "\n",
        " **This is because our corpus is small**.\n",
        " \n",
        "In order to get more meaningful results, we need to train word2vec representations using much larger corpuses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70Qfzs6N2y0M",
        "colab_type": "text"
      },
      "source": [
        "Now, **let's create our numerical features using the word2vec representations of the words**. \n",
        "\n",
        "**In the following, we get the word2vec vectors of each word in a sentence and take the average of all the vectors in the high dimensional space** (in our case it's 100).\n",
        "\n",
        "**So, as a result, we'll have a vector of 100 dimensions as the feature for a sentence**.\n",
        " \n",
        " **We then use each dimension as a separate feature**\n",
        " \n",
        "  **which means that in our final dataset we'll have 100 numerical feature**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90eNGKng3HKR",
        "colab_type": "code",
        "outputId": "316b9523-eb0d-4aed-85a5-c4cbb499f8c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "word2vec_arr = np.zeros((sentences.shape[0],100))\n",
        "\n",
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "    word2vec_arr[i,:] = np.mean([model[lemma] for lemma in sentence], axis=0)\n",
        "\n",
        "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
        "sentences = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
        "sentences.dropna(inplace=True)\n",
        "\n",
        "sentences.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[Alice, begin, tired, sit, sister, bank, have,...</td>\n",
              "      <td>-0.210979</td>\n",
              "      <td>-0.062580</td>\n",
              "      <td>0.157565</td>\n",
              "      <td>-0.139776</td>\n",
              "      <td>-0.300331</td>\n",
              "      <td>0.030455</td>\n",
              "      <td>0.077550</td>\n",
              "      <td>-0.294559</td>\n",
              "      <td>0.401140</td>\n",
              "      <td>0.053366</td>\n",
              "      <td>0.024731</td>\n",
              "      <td>0.196388</td>\n",
              "      <td>0.190764</td>\n",
              "      <td>0.308147</td>\n",
              "      <td>-0.063027</td>\n",
              "      <td>0.412527</td>\n",
              "      <td>0.139875</td>\n",
              "      <td>-0.267728</td>\n",
              "      <td>0.283771</td>\n",
              "      <td>-0.213635</td>\n",
              "      <td>-0.280361</td>\n",
              "      <td>0.244138</td>\n",
              "      <td>0.130352</td>\n",
              "      <td>-0.276069</td>\n",
              "      <td>-0.276648</td>\n",
              "      <td>-0.039348</td>\n",
              "      <td>0.227371</td>\n",
              "      <td>0.139667</td>\n",
              "      <td>0.073735</td>\n",
              "      <td>0.318960</td>\n",
              "      <td>0.166239</td>\n",
              "      <td>0.384208</td>\n",
              "      <td>-0.207219</td>\n",
              "      <td>0.036184</td>\n",
              "      <td>0.017213</td>\n",
              "      <td>0.101088</td>\n",
              "      <td>0.002001</td>\n",
              "      <td>-0.128893</td>\n",
              "      <td>...</td>\n",
              "      <td>0.265528</td>\n",
              "      <td>0.080171</td>\n",
              "      <td>0.069522</td>\n",
              "      <td>-0.210469</td>\n",
              "      <td>-0.218688</td>\n",
              "      <td>-0.071758</td>\n",
              "      <td>-0.008416</td>\n",
              "      <td>-0.210619</td>\n",
              "      <td>-0.424323</td>\n",
              "      <td>0.063695</td>\n",
              "      <td>0.327052</td>\n",
              "      <td>0.050011</td>\n",
              "      <td>-0.039167</td>\n",
              "      <td>0.440649</td>\n",
              "      <td>-0.325632</td>\n",
              "      <td>-0.036917</td>\n",
              "      <td>0.013718</td>\n",
              "      <td>0.249279</td>\n",
              "      <td>-0.154799</td>\n",
              "      <td>0.150525</td>\n",
              "      <td>-0.568831</td>\n",
              "      <td>0.460826</td>\n",
              "      <td>0.001292</td>\n",
              "      <td>0.688294</td>\n",
              "      <td>0.550359</td>\n",
              "      <td>0.074429</td>\n",
              "      <td>-0.079230</td>\n",
              "      <td>-0.062074</td>\n",
              "      <td>0.422813</td>\n",
              "      <td>0.180806</td>\n",
              "      <td>0.015324</td>\n",
              "      <td>0.042619</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>0.012137</td>\n",
              "      <td>0.023959</td>\n",
              "      <td>-0.027405</td>\n",
              "      <td>0.361840</td>\n",
              "      <td>-0.034830</td>\n",
              "      <td>0.025005</td>\n",
              "      <td>-0.131617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[consider, mind, hot, day, feel, sleepy, stupi...</td>\n",
              "      <td>-0.148910</td>\n",
              "      <td>-0.049488</td>\n",
              "      <td>0.153844</td>\n",
              "      <td>-0.123738</td>\n",
              "      <td>-0.238799</td>\n",
              "      <td>0.019646</td>\n",
              "      <td>0.064064</td>\n",
              "      <td>-0.233655</td>\n",
              "      <td>0.317688</td>\n",
              "      <td>0.043966</td>\n",
              "      <td>0.001401</td>\n",
              "      <td>0.168656</td>\n",
              "      <td>0.153475</td>\n",
              "      <td>0.232277</td>\n",
              "      <td>-0.047278</td>\n",
              "      <td>0.338224</td>\n",
              "      <td>0.122620</td>\n",
              "      <td>-0.215447</td>\n",
              "      <td>0.238220</td>\n",
              "      <td>-0.164725</td>\n",
              "      <td>-0.229579</td>\n",
              "      <td>0.206839</td>\n",
              "      <td>0.102739</td>\n",
              "      <td>-0.233687</td>\n",
              "      <td>-0.229472</td>\n",
              "      <td>-0.019650</td>\n",
              "      <td>0.185433</td>\n",
              "      <td>0.122994</td>\n",
              "      <td>0.062729</td>\n",
              "      <td>0.273803</td>\n",
              "      <td>0.125416</td>\n",
              "      <td>0.322074</td>\n",
              "      <td>-0.176348</td>\n",
              "      <td>0.046174</td>\n",
              "      <td>0.005779</td>\n",
              "      <td>0.084155</td>\n",
              "      <td>0.006412</td>\n",
              "      <td>-0.092547</td>\n",
              "      <td>...</td>\n",
              "      <td>0.210656</td>\n",
              "      <td>0.070094</td>\n",
              "      <td>0.061898</td>\n",
              "      <td>-0.178471</td>\n",
              "      <td>-0.168583</td>\n",
              "      <td>-0.054268</td>\n",
              "      <td>0.004315</td>\n",
              "      <td>-0.163944</td>\n",
              "      <td>-0.350977</td>\n",
              "      <td>0.057233</td>\n",
              "      <td>0.272216</td>\n",
              "      <td>0.044247</td>\n",
              "      <td>-0.035227</td>\n",
              "      <td>0.374482</td>\n",
              "      <td>-0.268617</td>\n",
              "      <td>-0.022345</td>\n",
              "      <td>0.014725</td>\n",
              "      <td>0.196182</td>\n",
              "      <td>-0.134059</td>\n",
              "      <td>0.128452</td>\n",
              "      <td>-0.461159</td>\n",
              "      <td>0.401367</td>\n",
              "      <td>-0.002373</td>\n",
              "      <td>0.556046</td>\n",
              "      <td>0.462653</td>\n",
              "      <td>0.049410</td>\n",
              "      <td>-0.073997</td>\n",
              "      <td>-0.072971</td>\n",
              "      <td>0.346400</td>\n",
              "      <td>0.149298</td>\n",
              "      <td>0.009316</td>\n",
              "      <td>0.039262</td>\n",
              "      <td>-0.005203</td>\n",
              "      <td>0.011666</td>\n",
              "      <td>0.011551</td>\n",
              "      <td>-0.033964</td>\n",
              "      <td>0.284794</td>\n",
              "      <td>-0.020304</td>\n",
              "      <td>0.015079</td>\n",
              "      <td>-0.098131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[remarkable, Alice, think, way, hear, Rabbit, ...</td>\n",
              "      <td>-0.246879</td>\n",
              "      <td>-0.068721</td>\n",
              "      <td>0.181590</td>\n",
              "      <td>-0.154642</td>\n",
              "      <td>-0.352796</td>\n",
              "      <td>0.028803</td>\n",
              "      <td>0.064865</td>\n",
              "      <td>-0.357944</td>\n",
              "      <td>0.468414</td>\n",
              "      <td>0.064683</td>\n",
              "      <td>0.037273</td>\n",
              "      <td>0.220390</td>\n",
              "      <td>0.234696</td>\n",
              "      <td>0.367887</td>\n",
              "      <td>-0.059296</td>\n",
              "      <td>0.487883</td>\n",
              "      <td>0.175900</td>\n",
              "      <td>-0.325933</td>\n",
              "      <td>0.332101</td>\n",
              "      <td>-0.234599</td>\n",
              "      <td>-0.332688</td>\n",
              "      <td>0.266216</td>\n",
              "      <td>0.162187</td>\n",
              "      <td>-0.324126</td>\n",
              "      <td>-0.330758</td>\n",
              "      <td>-0.048694</td>\n",
              "      <td>0.252641</td>\n",
              "      <td>0.166388</td>\n",
              "      <td>0.083734</td>\n",
              "      <td>0.372707</td>\n",
              "      <td>0.201353</td>\n",
              "      <td>0.436548</td>\n",
              "      <td>-0.249705</td>\n",
              "      <td>0.053970</td>\n",
              "      <td>0.028631</td>\n",
              "      <td>0.128952</td>\n",
              "      <td>0.004195</td>\n",
              "      <td>-0.161062</td>\n",
              "      <td>...</td>\n",
              "      <td>0.300029</td>\n",
              "      <td>0.089528</td>\n",
              "      <td>0.102752</td>\n",
              "      <td>-0.254452</td>\n",
              "      <td>-0.235483</td>\n",
              "      <td>-0.071454</td>\n",
              "      <td>-0.009369</td>\n",
              "      <td>-0.244593</td>\n",
              "      <td>-0.498783</td>\n",
              "      <td>0.082906</td>\n",
              "      <td>0.381511</td>\n",
              "      <td>0.049200</td>\n",
              "      <td>-0.050880</td>\n",
              "      <td>0.504575</td>\n",
              "      <td>-0.389542</td>\n",
              "      <td>-0.055890</td>\n",
              "      <td>0.026004</td>\n",
              "      <td>0.290325</td>\n",
              "      <td>-0.190111</td>\n",
              "      <td>0.179262</td>\n",
              "      <td>-0.668596</td>\n",
              "      <td>0.547679</td>\n",
              "      <td>-0.001092</td>\n",
              "      <td>0.811608</td>\n",
              "      <td>0.642291</td>\n",
              "      <td>0.086922</td>\n",
              "      <td>-0.090486</td>\n",
              "      <td>-0.079367</td>\n",
              "      <td>0.495971</td>\n",
              "      <td>0.217190</td>\n",
              "      <td>0.012852</td>\n",
              "      <td>0.054913</td>\n",
              "      <td>0.003135</td>\n",
              "      <td>0.016187</td>\n",
              "      <td>0.026923</td>\n",
              "      <td>-0.038698</td>\n",
              "      <td>0.422641</td>\n",
              "      <td>-0.051065</td>\n",
              "      <td>0.027931</td>\n",
              "      <td>-0.150146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[oh, dear]</td>\n",
              "      <td>-0.198752</td>\n",
              "      <td>-0.055472</td>\n",
              "      <td>0.189465</td>\n",
              "      <td>-0.127368</td>\n",
              "      <td>-0.291132</td>\n",
              "      <td>0.009599</td>\n",
              "      <td>0.060696</td>\n",
              "      <td>-0.323270</td>\n",
              "      <td>0.389814</td>\n",
              "      <td>0.072731</td>\n",
              "      <td>0.030511</td>\n",
              "      <td>0.188868</td>\n",
              "      <td>0.196643</td>\n",
              "      <td>0.344093</td>\n",
              "      <td>-0.058606</td>\n",
              "      <td>0.410212</td>\n",
              "      <td>0.165069</td>\n",
              "      <td>-0.258981</td>\n",
              "      <td>0.308424</td>\n",
              "      <td>-0.193313</td>\n",
              "      <td>-0.292446</td>\n",
              "      <td>0.235932</td>\n",
              "      <td>0.147183</td>\n",
              "      <td>-0.302798</td>\n",
              "      <td>-0.306351</td>\n",
              "      <td>-0.023146</td>\n",
              "      <td>0.227181</td>\n",
              "      <td>0.175893</td>\n",
              "      <td>0.038049</td>\n",
              "      <td>0.337455</td>\n",
              "      <td>0.177974</td>\n",
              "      <td>0.361876</td>\n",
              "      <td>-0.216765</td>\n",
              "      <td>0.057850</td>\n",
              "      <td>0.007536</td>\n",
              "      <td>0.126324</td>\n",
              "      <td>-0.004724</td>\n",
              "      <td>-0.125484</td>\n",
              "      <td>...</td>\n",
              "      <td>0.280289</td>\n",
              "      <td>0.080346</td>\n",
              "      <td>0.103590</td>\n",
              "      <td>-0.197471</td>\n",
              "      <td>-0.192529</td>\n",
              "      <td>-0.036362</td>\n",
              "      <td>0.019838</td>\n",
              "      <td>-0.210620</td>\n",
              "      <td>-0.419918</td>\n",
              "      <td>0.048789</td>\n",
              "      <td>0.322911</td>\n",
              "      <td>0.028338</td>\n",
              "      <td>-0.049335</td>\n",
              "      <td>0.437095</td>\n",
              "      <td>-0.343182</td>\n",
              "      <td>-0.032262</td>\n",
              "      <td>0.022310</td>\n",
              "      <td>0.241956</td>\n",
              "      <td>-0.186074</td>\n",
              "      <td>0.153712</td>\n",
              "      <td>-0.577593</td>\n",
              "      <td>0.490170</td>\n",
              "      <td>0.013413</td>\n",
              "      <td>0.715294</td>\n",
              "      <td>0.542974</td>\n",
              "      <td>0.097050</td>\n",
              "      <td>-0.079153</td>\n",
              "      <td>-0.072409</td>\n",
              "      <td>0.398773</td>\n",
              "      <td>0.176581</td>\n",
              "      <td>0.013658</td>\n",
              "      <td>0.047693</td>\n",
              "      <td>-0.003861</td>\n",
              "      <td>0.023843</td>\n",
              "      <td>0.029919</td>\n",
              "      <td>-0.019725</td>\n",
              "      <td>0.360995</td>\n",
              "      <td>-0.019371</td>\n",
              "      <td>0.030002</td>\n",
              "      <td>-0.113956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[shall, late]</td>\n",
              "      <td>-0.170687</td>\n",
              "      <td>-0.064303</td>\n",
              "      <td>0.120265</td>\n",
              "      <td>-0.105524</td>\n",
              "      <td>-0.256076</td>\n",
              "      <td>0.023947</td>\n",
              "      <td>0.064427</td>\n",
              "      <td>-0.234864</td>\n",
              "      <td>0.308412</td>\n",
              "      <td>0.032545</td>\n",
              "      <td>0.007218</td>\n",
              "      <td>0.161663</td>\n",
              "      <td>0.152126</td>\n",
              "      <td>0.237690</td>\n",
              "      <td>-0.047349</td>\n",
              "      <td>0.336657</td>\n",
              "      <td>0.101153</td>\n",
              "      <td>-0.218439</td>\n",
              "      <td>0.222103</td>\n",
              "      <td>-0.166517</td>\n",
              "      <td>-0.208742</td>\n",
              "      <td>0.180475</td>\n",
              "      <td>0.116417</td>\n",
              "      <td>-0.210406</td>\n",
              "      <td>-0.219503</td>\n",
              "      <td>-0.041713</td>\n",
              "      <td>0.174058</td>\n",
              "      <td>0.119881</td>\n",
              "      <td>0.071810</td>\n",
              "      <td>0.253760</td>\n",
              "      <td>0.136691</td>\n",
              "      <td>0.306267</td>\n",
              "      <td>-0.166559</td>\n",
              "      <td>0.015745</td>\n",
              "      <td>0.010220</td>\n",
              "      <td>0.073358</td>\n",
              "      <td>-0.014180</td>\n",
              "      <td>-0.101173</td>\n",
              "      <td>...</td>\n",
              "      <td>0.197397</td>\n",
              "      <td>0.067974</td>\n",
              "      <td>0.041263</td>\n",
              "      <td>-0.165409</td>\n",
              "      <td>-0.177770</td>\n",
              "      <td>-0.067592</td>\n",
              "      <td>-0.001795</td>\n",
              "      <td>-0.164673</td>\n",
              "      <td>-0.335656</td>\n",
              "      <td>0.062644</td>\n",
              "      <td>0.263830</td>\n",
              "      <td>0.043861</td>\n",
              "      <td>-0.023529</td>\n",
              "      <td>0.337201</td>\n",
              "      <td>-0.256994</td>\n",
              "      <td>-0.019061</td>\n",
              "      <td>0.008255</td>\n",
              "      <td>0.201162</td>\n",
              "      <td>-0.123421</td>\n",
              "      <td>0.119953</td>\n",
              "      <td>-0.445364</td>\n",
              "      <td>0.372372</td>\n",
              "      <td>-0.002701</td>\n",
              "      <td>0.538392</td>\n",
              "      <td>0.442920</td>\n",
              "      <td>0.059975</td>\n",
              "      <td>-0.058816</td>\n",
              "      <td>-0.054449</td>\n",
              "      <td>0.326140</td>\n",
              "      <td>0.151117</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>0.026683</td>\n",
              "      <td>-0.000501</td>\n",
              "      <td>0.011815</td>\n",
              "      <td>0.020486</td>\n",
              "      <td>-0.021360</td>\n",
              "      <td>0.287478</td>\n",
              "      <td>-0.018363</td>\n",
              "      <td>0.004685</td>\n",
              "      <td>-0.101913</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    author  ...        99\n",
              "0  Carroll  ... -0.131617\n",
              "1  Carroll  ... -0.098131\n",
              "2  Carroll  ... -0.150146\n",
              "3  Carroll  ... -0.113956\n",
              "4  Carroll  ... -0.101913\n",
              "\n",
              "[5 rows x 102 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU9tKtTS3ZZx",
        "colab_type": "text"
      },
      "source": [
        "**This is a dataset format that we like**.\n",
        "\n",
        " **Now, we're ready to jump into modeling step with our features**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4pDI-Gj3jYt",
        "colab_type": "text"
      },
      "source": [
        "##**Word2vec in action**\n",
        "\n",
        "**Notice that we now have a dataset where the columns named from 0 to 99 are the features we'll use in the following models**. \n",
        "\n",
        "We use the same models that we built in the previous checkpoints to predict the author of a sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puklpXE331Jk",
        "colab_type": "code",
        "outputId": "5ed0f0a3-a768-46cb-dbb2-3dba5550d15e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = sentences['author']\n",
        "X = np.array(sentences.drop(['text','author'], 1))\n",
        "\n",
        "# We split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
        "\n",
        "# Models\n",
        "lr = LogisticRegression()\n",
        "rfc = RandomForestClassifier()\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "rfc.fit(X_train, y_train)\n",
        "gbc.fit(X_train, y_train)\n",
        "\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train, y_train))\n",
        "print('\\nTest set score:', lr.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train, y_train))\n",
        "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train, y_train))\n",
        "print('\\nTest set score:', gbc.score(X_test, y_test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.7485540334855403\n",
            "\n",
            "Test set score: 0.7593607305936073\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9914764079147641\n",
            "\n",
            "Test set score: 0.7986301369863014\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.8867579908675799\n",
            "\n",
            "Test set score: 0.8082191780821918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnHcH2-a4wCP",
        "colab_type": "text"
      },
      "source": [
        "The scores aren't great compared to the scores of the previous checkpoints.\n",
        "\n",
        "**The main reason is the small size of our corpus**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfgeHTGX4-d9",
        "colab_type": "text"
      },
      "source": [
        "**So, let's use word2vec vectors that are trained on a very large corpus**.\n",
        "\n",
        " For this, we use pre-trained vectors released by Google. Google released a quite large word2vec vectors that are trained on around 100 billion words from Google News dataset. \n",
        " \n",
        " \n",
        "**Their corpus contains 3 million words and the word vectors they trained have 300 features each**.\n",
        "\n",
        "We'll download the **pre-trained vectors** from this address: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz. Note that the download and the following codes take some time. So, we recommend running the following cells in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQDgJehF5G2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Google's pre-trained Word2Vec model.\n",
        "model_pretrained = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "    'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5poxXbg5xG1",
        "colab_type": "text"
      },
      "source": [
        "**Now, we have the pre-trained vectors in a variable called model_pretrained**. \n",
        "\n",
        "**Next, we look for the vector representations of the words in our corpus**.\n",
        "\n",
        " **For simplicity, if a word in a sentence can't be found in the vocabulary of these pre-trained vectors,**\n",
        " \n",
        " **we just simply drop those sentences from our dataset**.\n",
        " \n",
        " ou can follow alternative approaches if you like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dzEgzxZ6Cbc",
        "colab_type": "code",
        "outputId": "3e0edde7-d163-48ed-86a3-fef148c56ace",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "word2vec_arr = np.zeros((sentences.shape[0],300))\n",
        "\n",
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "  try:\n",
        "    word2vec_arr[i,:] = np.mean([model_pretrained[lemma] for lemma in sentence], axis=0)\n",
        "  except KeyError:\n",
        "    word2vec_arr[i,:] = np.full((1,300), np.nan) #fill in with nan -- this vector will be dropped on line #12\n",
        "    continue\n",
        "\n",
        "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
        "sentences = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
        "sentences.dropna(inplace=True)\n",
        "\n",
        "print(\"Shape of the dataset: {}\".format(sentences.shape))\n",
        "sentences.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the dataset: (4610, 302)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>...</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[Alice, begin, tired, sit, sister, bank, have,...</td>\n",
              "      <td>0.046265</td>\n",
              "      <td>0.016199</td>\n",
              "      <td>-0.036288</td>\n",
              "      <td>0.082410</td>\n",
              "      <td>-0.010284</td>\n",
              "      <td>0.015515</td>\n",
              "      <td>0.005437</td>\n",
              "      <td>-0.035947</td>\n",
              "      <td>0.067871</td>\n",
              "      <td>0.040186</td>\n",
              "      <td>0.002303</td>\n",
              "      <td>-0.071809</td>\n",
              "      <td>-0.002277</td>\n",
              "      <td>0.035602</td>\n",
              "      <td>-0.087659</td>\n",
              "      <td>0.067581</td>\n",
              "      <td>0.083479</td>\n",
              "      <td>0.109125</td>\n",
              "      <td>0.038206</td>\n",
              "      <td>-0.112296</td>\n",
              "      <td>0.021118</td>\n",
              "      <td>0.067197</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>-0.046423</td>\n",
              "      <td>0.030869</td>\n",
              "      <td>0.000274</td>\n",
              "      <td>-0.084494</td>\n",
              "      <td>0.078152</td>\n",
              "      <td>0.047164</td>\n",
              "      <td>-0.023407</td>\n",
              "      <td>-0.105367</td>\n",
              "      <td>-0.039990</td>\n",
              "      <td>-0.110767</td>\n",
              "      <td>-0.065475</td>\n",
              "      <td>0.023956</td>\n",
              "      <td>0.010934</td>\n",
              "      <td>0.094955</td>\n",
              "      <td>0.015027</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.055796</td>\n",
              "      <td>0.055115</td>\n",
              "      <td>-0.117415</td>\n",
              "      <td>-0.030527</td>\n",
              "      <td>-0.015355</td>\n",
              "      <td>0.163165</td>\n",
              "      <td>-0.034854</td>\n",
              "      <td>0.015172</td>\n",
              "      <td>-0.106117</td>\n",
              "      <td>0.035062</td>\n",
              "      <td>0.086723</td>\n",
              "      <td>0.159433</td>\n",
              "      <td>0.103741</td>\n",
              "      <td>0.062915</td>\n",
              "      <td>0.097021</td>\n",
              "      <td>-0.047644</td>\n",
              "      <td>-0.026648</td>\n",
              "      <td>-0.071558</td>\n",
              "      <td>0.018080</td>\n",
              "      <td>-0.039431</td>\n",
              "      <td>0.121521</td>\n",
              "      <td>-0.125867</td>\n",
              "      <td>0.006816</td>\n",
              "      <td>0.029865</td>\n",
              "      <td>0.046413</td>\n",
              "      <td>0.018112</td>\n",
              "      <td>-0.087307</td>\n",
              "      <td>0.042181</td>\n",
              "      <td>-0.015435</td>\n",
              "      <td>0.128412</td>\n",
              "      <td>-0.066516</td>\n",
              "      <td>0.029852</td>\n",
              "      <td>-0.042609</td>\n",
              "      <td>-0.044208</td>\n",
              "      <td>-0.056998</td>\n",
              "      <td>-0.063269</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>-0.085071</td>\n",
              "      <td>-0.000340</td>\n",
              "      <td>-0.064371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[consider, mind, hot, day, feel, sleepy, stupi...</td>\n",
              "      <td>0.046331</td>\n",
              "      <td>0.020463</td>\n",
              "      <td>-0.002012</td>\n",
              "      <td>0.101565</td>\n",
              "      <td>-0.066478</td>\n",
              "      <td>-0.035698</td>\n",
              "      <td>0.045293</td>\n",
              "      <td>-0.068695</td>\n",
              "      <td>0.044050</td>\n",
              "      <td>0.079996</td>\n",
              "      <td>0.010562</td>\n",
              "      <td>-0.098240</td>\n",
              "      <td>-0.024309</td>\n",
              "      <td>0.042576</td>\n",
              "      <td>-0.078658</td>\n",
              "      <td>0.026042</td>\n",
              "      <td>-0.025208</td>\n",
              "      <td>0.128391</td>\n",
              "      <td>0.054481</td>\n",
              "      <td>-0.081564</td>\n",
              "      <td>-0.022604</td>\n",
              "      <td>0.060187</td>\n",
              "      <td>0.014813</td>\n",
              "      <td>-0.002640</td>\n",
              "      <td>0.089216</td>\n",
              "      <td>0.010905</td>\n",
              "      <td>-0.080477</td>\n",
              "      <td>0.078742</td>\n",
              "      <td>0.071459</td>\n",
              "      <td>-0.042953</td>\n",
              "      <td>-0.011639</td>\n",
              "      <td>0.026516</td>\n",
              "      <td>-0.042924</td>\n",
              "      <td>-0.028997</td>\n",
              "      <td>-0.010134</td>\n",
              "      <td>-0.033885</td>\n",
              "      <td>0.051852</td>\n",
              "      <td>0.018926</td>\n",
              "      <td>...</td>\n",
              "      <td>0.037855</td>\n",
              "      <td>0.004276</td>\n",
              "      <td>-0.073813</td>\n",
              "      <td>0.033909</td>\n",
              "      <td>0.053077</td>\n",
              "      <td>0.063299</td>\n",
              "      <td>-0.044852</td>\n",
              "      <td>-0.004278</td>\n",
              "      <td>-0.053132</td>\n",
              "      <td>-0.035156</td>\n",
              "      <td>0.047930</td>\n",
              "      <td>0.126340</td>\n",
              "      <td>0.125036</td>\n",
              "      <td>0.046570</td>\n",
              "      <td>0.049766</td>\n",
              "      <td>-0.076279</td>\n",
              "      <td>-0.069141</td>\n",
              "      <td>-0.122912</td>\n",
              "      <td>-0.052948</td>\n",
              "      <td>0.055787</td>\n",
              "      <td>0.081729</td>\n",
              "      <td>0.011096</td>\n",
              "      <td>0.005422</td>\n",
              "      <td>0.050716</td>\n",
              "      <td>-0.050148</td>\n",
              "      <td>-0.008294</td>\n",
              "      <td>-0.072707</td>\n",
              "      <td>-0.002824</td>\n",
              "      <td>0.021307</td>\n",
              "      <td>0.035784</td>\n",
              "      <td>0.055940</td>\n",
              "      <td>0.085838</td>\n",
              "      <td>-0.067052</td>\n",
              "      <td>-0.013628</td>\n",
              "      <td>-0.027802</td>\n",
              "      <td>-0.033665</td>\n",
              "      <td>-0.023586</td>\n",
              "      <td>0.009620</td>\n",
              "      <td>0.030316</td>\n",
              "      <td>0.000908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[remarkable, Alice, think, way, hear, Rabbit, ...</td>\n",
              "      <td>0.061646</td>\n",
              "      <td>-0.006958</td>\n",
              "      <td>-0.013023</td>\n",
              "      <td>0.147003</td>\n",
              "      <td>-0.052933</td>\n",
              "      <td>-0.077866</td>\n",
              "      <td>0.033997</td>\n",
              "      <td>-0.061890</td>\n",
              "      <td>0.104706</td>\n",
              "      <td>0.151611</td>\n",
              "      <td>-0.083191</td>\n",
              "      <td>-0.102318</td>\n",
              "      <td>-0.043243</td>\n",
              "      <td>-0.060654</td>\n",
              "      <td>-0.060211</td>\n",
              "      <td>0.105164</td>\n",
              "      <td>0.127869</td>\n",
              "      <td>0.207825</td>\n",
              "      <td>-0.009186</td>\n",
              "      <td>0.009155</td>\n",
              "      <td>0.005402</td>\n",
              "      <td>0.077332</td>\n",
              "      <td>0.129974</td>\n",
              "      <td>-0.026632</td>\n",
              "      <td>0.149017</td>\n",
              "      <td>0.043540</td>\n",
              "      <td>-0.082504</td>\n",
              "      <td>0.020443</td>\n",
              "      <td>0.117149</td>\n",
              "      <td>-0.014988</td>\n",
              "      <td>-0.064789</td>\n",
              "      <td>-0.023331</td>\n",
              "      <td>-0.068970</td>\n",
              "      <td>0.002205</td>\n",
              "      <td>0.015739</td>\n",
              "      <td>0.018581</td>\n",
              "      <td>0.110168</td>\n",
              "      <td>0.057068</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.073837</td>\n",
              "      <td>-0.021027</td>\n",
              "      <td>0.002594</td>\n",
              "      <td>0.025757</td>\n",
              "      <td>-0.004457</td>\n",
              "      <td>0.067825</td>\n",
              "      <td>-0.060242</td>\n",
              "      <td>-0.063232</td>\n",
              "      <td>-0.079094</td>\n",
              "      <td>0.098316</td>\n",
              "      <td>0.021147</td>\n",
              "      <td>0.124046</td>\n",
              "      <td>0.078278</td>\n",
              "      <td>0.056248</td>\n",
              "      <td>0.099792</td>\n",
              "      <td>-0.106703</td>\n",
              "      <td>0.034882</td>\n",
              "      <td>-0.111328</td>\n",
              "      <td>-0.009624</td>\n",
              "      <td>-0.011642</td>\n",
              "      <td>0.088547</td>\n",
              "      <td>-0.059265</td>\n",
              "      <td>-0.041046</td>\n",
              "      <td>0.069794</td>\n",
              "      <td>-0.002939</td>\n",
              "      <td>0.018978</td>\n",
              "      <td>-0.025116</td>\n",
              "      <td>-0.057938</td>\n",
              "      <td>0.007706</td>\n",
              "      <td>0.120476</td>\n",
              "      <td>-0.006882</td>\n",
              "      <td>0.030754</td>\n",
              "      <td>-0.073837</td>\n",
              "      <td>-0.010359</td>\n",
              "      <td>-0.086411</td>\n",
              "      <td>-0.156464</td>\n",
              "      <td>-0.000771</td>\n",
              "      <td>-0.000549</td>\n",
              "      <td>-0.003784</td>\n",
              "      <td>0.029114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[oh, dear]</td>\n",
              "      <td>0.073975</td>\n",
              "      <td>0.134277</td>\n",
              "      <td>0.141357</td>\n",
              "      <td>0.256348</td>\n",
              "      <td>-0.147949</td>\n",
              "      <td>0.099670</td>\n",
              "      <td>0.077148</td>\n",
              "      <td>-0.093628</td>\n",
              "      <td>0.108887</td>\n",
              "      <td>0.281738</td>\n",
              "      <td>-0.201172</td>\n",
              "      <td>-0.020752</td>\n",
              "      <td>-0.266602</td>\n",
              "      <td>0.000732</td>\n",
              "      <td>-0.036865</td>\n",
              "      <td>0.294434</td>\n",
              "      <td>0.158203</td>\n",
              "      <td>0.287109</td>\n",
              "      <td>-0.114624</td>\n",
              "      <td>0.038330</td>\n",
              "      <td>0.141357</td>\n",
              "      <td>-0.046021</td>\n",
              "      <td>0.407227</td>\n",
              "      <td>0.047852</td>\n",
              "      <td>0.322266</td>\n",
              "      <td>0.213379</td>\n",
              "      <td>-0.090576</td>\n",
              "      <td>0.022812</td>\n",
              "      <td>0.171265</td>\n",
              "      <td>-0.283203</td>\n",
              "      <td>0.193848</td>\n",
              "      <td>0.092285</td>\n",
              "      <td>-0.122803</td>\n",
              "      <td>0.029770</td>\n",
              "      <td>-0.116943</td>\n",
              "      <td>0.026123</td>\n",
              "      <td>0.137451</td>\n",
              "      <td>0.055298</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.014648</td>\n",
              "      <td>0.112793</td>\n",
              "      <td>0.071716</td>\n",
              "      <td>-0.133911</td>\n",
              "      <td>-0.091553</td>\n",
              "      <td>-0.079041</td>\n",
              "      <td>-0.156250</td>\n",
              "      <td>-0.029053</td>\n",
              "      <td>-0.024719</td>\n",
              "      <td>0.102844</td>\n",
              "      <td>-0.084473</td>\n",
              "      <td>0.163086</td>\n",
              "      <td>-0.031738</td>\n",
              "      <td>-0.084473</td>\n",
              "      <td>0.149170</td>\n",
              "      <td>-0.082031</td>\n",
              "      <td>-0.023438</td>\n",
              "      <td>-0.199219</td>\n",
              "      <td>-0.253418</td>\n",
              "      <td>0.206055</td>\n",
              "      <td>0.160156</td>\n",
              "      <td>-0.056030</td>\n",
              "      <td>-0.138184</td>\n",
              "      <td>0.208496</td>\n",
              "      <td>0.030762</td>\n",
              "      <td>0.033447</td>\n",
              "      <td>-0.061890</td>\n",
              "      <td>-0.022461</td>\n",
              "      <td>-0.146240</td>\n",
              "      <td>-0.032959</td>\n",
              "      <td>0.058228</td>\n",
              "      <td>0.000854</td>\n",
              "      <td>-0.094971</td>\n",
              "      <td>-0.052668</td>\n",
              "      <td>-0.091919</td>\n",
              "      <td>-0.142456</td>\n",
              "      <td>-0.053711</td>\n",
              "      <td>-0.112671</td>\n",
              "      <td>-0.148193</td>\n",
              "      <td>0.186798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[shall, late]</td>\n",
              "      <td>0.095215</td>\n",
              "      <td>0.084473</td>\n",
              "      <td>0.206787</td>\n",
              "      <td>0.211182</td>\n",
              "      <td>0.043579</td>\n",
              "      <td>-0.155762</td>\n",
              "      <td>0.088379</td>\n",
              "      <td>-0.038574</td>\n",
              "      <td>0.065613</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>-0.144287</td>\n",
              "      <td>0.001465</td>\n",
              "      <td>-0.000771</td>\n",
              "      <td>0.189453</td>\n",
              "      <td>-0.058350</td>\n",
              "      <td>-0.062134</td>\n",
              "      <td>0.045898</td>\n",
              "      <td>0.130127</td>\n",
              "      <td>0.211426</td>\n",
              "      <td>0.074341</td>\n",
              "      <td>-0.056122</td>\n",
              "      <td>-0.111145</td>\n",
              "      <td>0.104355</td>\n",
              "      <td>0.069946</td>\n",
              "      <td>0.191895</td>\n",
              "      <td>0.057404</td>\n",
              "      <td>-0.003906</td>\n",
              "      <td>0.107666</td>\n",
              "      <td>-0.040039</td>\n",
              "      <td>0.082275</td>\n",
              "      <td>-0.046707</td>\n",
              "      <td>-0.150635</td>\n",
              "      <td>-0.006226</td>\n",
              "      <td>0.048950</td>\n",
              "      <td>-0.088745</td>\n",
              "      <td>0.088501</td>\n",
              "      <td>-0.081573</td>\n",
              "      <td>-0.180542</td>\n",
              "      <td>...</td>\n",
              "      <td>0.133301</td>\n",
              "      <td>0.074219</td>\n",
              "      <td>0.049438</td>\n",
              "      <td>0.092743</td>\n",
              "      <td>0.077618</td>\n",
              "      <td>0.084229</td>\n",
              "      <td>-0.100586</td>\n",
              "      <td>-0.022217</td>\n",
              "      <td>0.043579</td>\n",
              "      <td>-0.029785</td>\n",
              "      <td>0.212158</td>\n",
              "      <td>0.073242</td>\n",
              "      <td>0.100220</td>\n",
              "      <td>0.062256</td>\n",
              "      <td>0.167480</td>\n",
              "      <td>0.010693</td>\n",
              "      <td>-0.139923</td>\n",
              "      <td>-0.013805</td>\n",
              "      <td>-0.127014</td>\n",
              "      <td>0.001465</td>\n",
              "      <td>-0.120972</td>\n",
              "      <td>0.063080</td>\n",
              "      <td>-0.024597</td>\n",
              "      <td>0.027847</td>\n",
              "      <td>0.010254</td>\n",
              "      <td>-0.073547</td>\n",
              "      <td>0.100098</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.107178</td>\n",
              "      <td>0.065918</td>\n",
              "      <td>-0.021667</td>\n",
              "      <td>-0.103516</td>\n",
              "      <td>-0.038578</td>\n",
              "      <td>-0.007385</td>\n",
              "      <td>0.020264</td>\n",
              "      <td>0.134155</td>\n",
              "      <td>-0.177246</td>\n",
              "      <td>-0.254639</td>\n",
              "      <td>-0.212158</td>\n",
              "      <td>0.087646</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 302 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    author  ...       299\n",
              "0  Carroll  ... -0.064371\n",
              "1  Carroll  ...  0.000908\n",
              "2  Carroll  ...  0.029114\n",
              "3  Carroll  ...  0.186798\n",
              "4  Carroll  ...  0.087646\n",
              "\n",
              "[5 rows x 302 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yxyUsbF7I-j",
        "colab_type": "text"
      },
      "source": [
        "As a result, we have a dataset of 4114 rows and 300 features (excluding the text and the author columns).\n",
        "\n",
        "Now, we can run our classifiers using this dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M--TAofB7RaJ",
        "colab_type": "text"
      },
      "source": [
        "**SEEMS EACH RUN PRODUCES A SLIGHTLY DIFF NUMBER OF ROWS**\n",
        "\n",
        "**OR ELSE CORPUS HAS BEEN UPDATED**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK5SY0g-7mb1",
        "colab_type": "code",
        "outputId": "c3970670-e40e-476c-8a28-df6eddfb40d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = sentences['author']\n",
        "X = np.array(sentences.drop(['text','author'], 1))\n",
        "\n",
        "# We split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
        "\n",
        "# Models\n",
        "lr = LogisticRegression()\n",
        "rfc = RandomForestClassifier()\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "rfc.fit(X_train, y_train)\n",
        "gbc.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
              "                           max_features=None, max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=1, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                           n_iter_no_change=None, presort='deprecated',\n",
              "                           random_state=None, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtOzqtEj74TO",
        "colab_type": "code",
        "outputId": "f932a4c6-b619-4b77-8f36-a741d014b19f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train, y_train))\n",
        "print('\\nTest set score:', lr.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train, y_train))\n",
        "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train, y_train))\n",
        "print('\\nTest set score:', gbc.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.8897324656543746\n",
            "\n",
            "Test set score: 0.8449023861171366\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9924078091106291\n",
            "\n",
            "Test set score: 0.7977223427331888\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.9508315256688359\n",
            "\n",
            "Test set score: 0.8156182212581344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_zmpP5R7_Hf",
        "colab_type": "text"
      },
      "source": [
        "**IMPROVED RESULTS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK0k1G5H8LXH",
        "colab_type": "text"
      },
      "source": [
        "#**ASSIGNMENTS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtWRCqhj8RBS",
        "colab_type": "text"
      },
      "source": [
        "**Train your own word2vec representations** as we did in our first example in the checkpoint.\n",
        "\n",
        "But, you need to experiment with the hyperparameters of the vectorization step\n",
        "\n",
        "**Modify the hyperparameters and run the classification models again**\n",
        "\n",
        "**Can you wrangle any improvements**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-YRXfbF8sc7",
        "colab_type": "text"
      },
      "source": [
        "**MAYBE, ONCE  I UNDERSTAND THE PARAMETERS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_V98ExK9DV8",
        "colab_type": "text"
      },
      "source": [
        "**HERE ARE ORIGINAL PARAMETERS:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqkp7Yrzjr7c",
        "colab_type": "text"
      },
      "source": [
        "The Word2Vec class has several parameters.\n",
        "\n",
        "We set the following parameters:\n",
        "\n",
        "**workers**=4: We set the number of threads to run in parallel to 4 (make sense if your computer has available computing units).\n",
        "\n",
        "**min_count**=1: We set the minimum word count threshold to 1.\n",
        "\n",
        "**window**=6: We set the number of words around target word to consider to 6.\n",
        "\n",
        "**sg**=0: We use CBOW because our corpus is small.\n",
        "\n",
        "**sample**=1e-3: We penalize frequent words.\n",
        "\n",
        "**size**=100: We set the word vector length to 100.\n",
        "\n",
        "**hs**=1: We use hierarchical softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Tn12CrFmbEG",
        "colab_type": "text"
      },
      "source": [
        "I WILL CHANGE A FEW:\n",
        "\n",
        "SG=1  SKIP-GRAM\n",
        "\n",
        "HS=0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5YKs_uhj0xN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train word2vec on the the sentences\n",
        "model = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=6,\n",
        "    sg=1,\n",
        "    sample=1e-3,\n",
        "    size=100,\n",
        "    hs=0\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYoHME11oP6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "5bc09575-1099-413a-92b8-40a0b120fd65"
      },
      "source": [
        "print(model.most_similar(positive=['lady', 'man'], negative=['woman'], topn=5))\n",
        "print(model.doesnt_match(\"dad dinner mom aunt uncle\".split()))\n",
        "print(model.similarity('woman', 'man'))\n",
        "print(model.similarity('horse', 'cat'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('brother', 0.9980732798576355), ('come', 0.99798583984375), ('sight', 0.9979526400566101), ('join', 0.997667133808136), ('Laconia', 0.9976547360420227)]\n",
            "uncle\n",
            "0.998033\n",
            "0.9990655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at63MS6koc4n",
        "colab_type": "text"
      },
      "source": [
        "**QUITE DIFFERENT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2mL1IApog7e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "b08b7b0e-fdc9-4d75-a6b4-c9646acc8309"
      },
      "source": [
        "word2vec_arr = np.zeros((sentences.shape[0],100))\n",
        "\n",
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "    word2vec_arr[i,:] = np.mean([model[lemma] for lemma in sentence], axis=0)\n",
        "\n",
        "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
        "sentences = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
        "sentences.dropna(inplace=True)\n",
        "\n",
        "sentences.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[Alice, begin, tired, sit, sister, bank, have,...</td>\n",
              "      <td>-0.086738</td>\n",
              "      <td>-0.066973</td>\n",
              "      <td>0.194718</td>\n",
              "      <td>-0.038142</td>\n",
              "      <td>-0.092576</td>\n",
              "      <td>0.025404</td>\n",
              "      <td>-0.097356</td>\n",
              "      <td>-0.069098</td>\n",
              "      <td>0.344794</td>\n",
              "      <td>0.014709</td>\n",
              "      <td>-0.113208</td>\n",
              "      <td>0.086345</td>\n",
              "      <td>0.065842</td>\n",
              "      <td>0.111541</td>\n",
              "      <td>0.020347</td>\n",
              "      <td>0.383489</td>\n",
              "      <td>0.042940</td>\n",
              "      <td>-0.022669</td>\n",
              "      <td>0.145239</td>\n",
              "      <td>-0.206149</td>\n",
              "      <td>-0.358930</td>\n",
              "      <td>0.064531</td>\n",
              "      <td>-0.004138</td>\n",
              "      <td>-0.196842</td>\n",
              "      <td>-0.094288</td>\n",
              "      <td>-0.043455</td>\n",
              "      <td>0.080698</td>\n",
              "      <td>0.168811</td>\n",
              "      <td>0.099269</td>\n",
              "      <td>0.063211</td>\n",
              "      <td>-0.107292</td>\n",
              "      <td>0.240973</td>\n",
              "      <td>-0.115087</td>\n",
              "      <td>0.120025</td>\n",
              "      <td>-0.068237</td>\n",
              "      <td>-0.044756</td>\n",
              "      <td>-0.075345</td>\n",
              "      <td>-0.164414</td>\n",
              "      <td>...</td>\n",
              "      <td>0.067016</td>\n",
              "      <td>-0.042553</td>\n",
              "      <td>-0.051143</td>\n",
              "      <td>-0.095098</td>\n",
              "      <td>-0.144280</td>\n",
              "      <td>-0.175392</td>\n",
              "      <td>0.139980</td>\n",
              "      <td>-0.138948</td>\n",
              "      <td>-0.326689</td>\n",
              "      <td>-0.007645</td>\n",
              "      <td>0.301480</td>\n",
              "      <td>-0.021835</td>\n",
              "      <td>-0.026336</td>\n",
              "      <td>0.185951</td>\n",
              "      <td>-0.297374</td>\n",
              "      <td>0.017580</td>\n",
              "      <td>0.043338</td>\n",
              "      <td>0.190664</td>\n",
              "      <td>-0.153402</td>\n",
              "      <td>0.026305</td>\n",
              "      <td>-0.474025</td>\n",
              "      <td>0.226737</td>\n",
              "      <td>0.074406</td>\n",
              "      <td>0.277472</td>\n",
              "      <td>0.239781</td>\n",
              "      <td>0.005632</td>\n",
              "      <td>-0.151593</td>\n",
              "      <td>-0.079029</td>\n",
              "      <td>0.165642</td>\n",
              "      <td>-0.024022</td>\n",
              "      <td>-0.016095</td>\n",
              "      <td>0.008199</td>\n",
              "      <td>-0.024706</td>\n",
              "      <td>-0.034492</td>\n",
              "      <td>-0.004901</td>\n",
              "      <td>-0.141160</td>\n",
              "      <td>0.090249</td>\n",
              "      <td>-0.147267</td>\n",
              "      <td>-0.071003</td>\n",
              "      <td>-0.095955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[consider, mind, hot, day, feel, sleepy, stupi...</td>\n",
              "      <td>-0.079642</td>\n",
              "      <td>-0.058659</td>\n",
              "      <td>0.176495</td>\n",
              "      <td>-0.034304</td>\n",
              "      <td>-0.080510</td>\n",
              "      <td>0.019714</td>\n",
              "      <td>-0.088960</td>\n",
              "      <td>-0.059914</td>\n",
              "      <td>0.306890</td>\n",
              "      <td>0.012223</td>\n",
              "      <td>-0.099544</td>\n",
              "      <td>0.077402</td>\n",
              "      <td>0.054977</td>\n",
              "      <td>0.097927</td>\n",
              "      <td>0.020270</td>\n",
              "      <td>0.345209</td>\n",
              "      <td>0.039257</td>\n",
              "      <td>-0.023266</td>\n",
              "      <td>0.131896</td>\n",
              "      <td>-0.183358</td>\n",
              "      <td>-0.318768</td>\n",
              "      <td>0.059231</td>\n",
              "      <td>-0.004875</td>\n",
              "      <td>-0.175451</td>\n",
              "      <td>-0.087784</td>\n",
              "      <td>-0.040046</td>\n",
              "      <td>0.072015</td>\n",
              "      <td>0.147458</td>\n",
              "      <td>0.089864</td>\n",
              "      <td>0.056492</td>\n",
              "      <td>-0.095106</td>\n",
              "      <td>0.215921</td>\n",
              "      <td>-0.101821</td>\n",
              "      <td>0.109779</td>\n",
              "      <td>-0.060913</td>\n",
              "      <td>-0.042119</td>\n",
              "      <td>-0.066199</td>\n",
              "      <td>-0.149445</td>\n",
              "      <td>...</td>\n",
              "      <td>0.059179</td>\n",
              "      <td>-0.036995</td>\n",
              "      <td>-0.044467</td>\n",
              "      <td>-0.084120</td>\n",
              "      <td>-0.129202</td>\n",
              "      <td>-0.154671</td>\n",
              "      <td>0.122542</td>\n",
              "      <td>-0.125052</td>\n",
              "      <td>-0.292851</td>\n",
              "      <td>-0.003369</td>\n",
              "      <td>0.269887</td>\n",
              "      <td>-0.019103</td>\n",
              "      <td>-0.024409</td>\n",
              "      <td>0.165669</td>\n",
              "      <td>-0.263451</td>\n",
              "      <td>0.019385</td>\n",
              "      <td>0.041925</td>\n",
              "      <td>0.171544</td>\n",
              "      <td>-0.134998</td>\n",
              "      <td>0.018321</td>\n",
              "      <td>-0.423333</td>\n",
              "      <td>0.200755</td>\n",
              "      <td>0.067218</td>\n",
              "      <td>0.249756</td>\n",
              "      <td>0.215810</td>\n",
              "      <td>0.004900</td>\n",
              "      <td>-0.138936</td>\n",
              "      <td>-0.070444</td>\n",
              "      <td>0.152848</td>\n",
              "      <td>-0.022898</td>\n",
              "      <td>-0.015348</td>\n",
              "      <td>0.002264</td>\n",
              "      <td>-0.023232</td>\n",
              "      <td>-0.031777</td>\n",
              "      <td>-0.004464</td>\n",
              "      <td>-0.125953</td>\n",
              "      <td>0.078503</td>\n",
              "      <td>-0.132127</td>\n",
              "      <td>-0.062781</td>\n",
              "      <td>-0.082511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[remarkable, Alice, think, way, hear, Rabbit, ...</td>\n",
              "      <td>-0.085359</td>\n",
              "      <td>-0.064368</td>\n",
              "      <td>0.194125</td>\n",
              "      <td>-0.031936</td>\n",
              "      <td>-0.089395</td>\n",
              "      <td>0.025563</td>\n",
              "      <td>-0.096264</td>\n",
              "      <td>-0.063209</td>\n",
              "      <td>0.336730</td>\n",
              "      <td>0.016716</td>\n",
              "      <td>-0.111266</td>\n",
              "      <td>0.084056</td>\n",
              "      <td>0.063879</td>\n",
              "      <td>0.109655</td>\n",
              "      <td>0.018697</td>\n",
              "      <td>0.376483</td>\n",
              "      <td>0.044901</td>\n",
              "      <td>-0.022722</td>\n",
              "      <td>0.145158</td>\n",
              "      <td>-0.201431</td>\n",
              "      <td>-0.352950</td>\n",
              "      <td>0.067148</td>\n",
              "      <td>-0.003106</td>\n",
              "      <td>-0.197068</td>\n",
              "      <td>-0.095209</td>\n",
              "      <td>-0.041725</td>\n",
              "      <td>0.078653</td>\n",
              "      <td>0.169217</td>\n",
              "      <td>0.097441</td>\n",
              "      <td>0.058873</td>\n",
              "      <td>-0.107858</td>\n",
              "      <td>0.235408</td>\n",
              "      <td>-0.116406</td>\n",
              "      <td>0.116216</td>\n",
              "      <td>-0.064614</td>\n",
              "      <td>-0.039719</td>\n",
              "      <td>-0.072207</td>\n",
              "      <td>-0.165831</td>\n",
              "      <td>...</td>\n",
              "      <td>0.066653</td>\n",
              "      <td>-0.040617</td>\n",
              "      <td>-0.048163</td>\n",
              "      <td>-0.091763</td>\n",
              "      <td>-0.140608</td>\n",
              "      <td>-0.172839</td>\n",
              "      <td>0.136259</td>\n",
              "      <td>-0.134173</td>\n",
              "      <td>-0.323789</td>\n",
              "      <td>-0.006166</td>\n",
              "      <td>0.296192</td>\n",
              "      <td>-0.025860</td>\n",
              "      <td>-0.025603</td>\n",
              "      <td>0.179698</td>\n",
              "      <td>-0.293718</td>\n",
              "      <td>0.021020</td>\n",
              "      <td>0.036755</td>\n",
              "      <td>0.194217</td>\n",
              "      <td>-0.151813</td>\n",
              "      <td>0.027326</td>\n",
              "      <td>-0.470922</td>\n",
              "      <td>0.226293</td>\n",
              "      <td>0.075123</td>\n",
              "      <td>0.272330</td>\n",
              "      <td>0.237724</td>\n",
              "      <td>0.002797</td>\n",
              "      <td>-0.143957</td>\n",
              "      <td>-0.076928</td>\n",
              "      <td>0.160650</td>\n",
              "      <td>-0.023743</td>\n",
              "      <td>-0.017215</td>\n",
              "      <td>0.008949</td>\n",
              "      <td>-0.021102</td>\n",
              "      <td>-0.035560</td>\n",
              "      <td>-0.002433</td>\n",
              "      <td>-0.141290</td>\n",
              "      <td>0.092959</td>\n",
              "      <td>-0.143728</td>\n",
              "      <td>-0.068994</td>\n",
              "      <td>-0.095584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[oh, dear]</td>\n",
              "      <td>-0.096373</td>\n",
              "      <td>-0.064530</td>\n",
              "      <td>0.197045</td>\n",
              "      <td>-0.045525</td>\n",
              "      <td>-0.086379</td>\n",
              "      <td>0.018699</td>\n",
              "      <td>-0.110723</td>\n",
              "      <td>-0.070469</td>\n",
              "      <td>0.354138</td>\n",
              "      <td>0.009603</td>\n",
              "      <td>-0.111456</td>\n",
              "      <td>0.087074</td>\n",
              "      <td>0.070432</td>\n",
              "      <td>0.119396</td>\n",
              "      <td>0.014106</td>\n",
              "      <td>0.390554</td>\n",
              "      <td>0.052142</td>\n",
              "      <td>-0.016757</td>\n",
              "      <td>0.151679</td>\n",
              "      <td>-0.209539</td>\n",
              "      <td>-0.366393</td>\n",
              "      <td>0.067420</td>\n",
              "      <td>-0.008687</td>\n",
              "      <td>-0.207316</td>\n",
              "      <td>-0.105511</td>\n",
              "      <td>-0.045703</td>\n",
              "      <td>0.078528</td>\n",
              "      <td>0.172799</td>\n",
              "      <td>0.101191</td>\n",
              "      <td>0.057001</td>\n",
              "      <td>-0.102920</td>\n",
              "      <td>0.239332</td>\n",
              "      <td>-0.118886</td>\n",
              "      <td>0.128884</td>\n",
              "      <td>-0.070392</td>\n",
              "      <td>-0.034584</td>\n",
              "      <td>-0.078401</td>\n",
              "      <td>-0.168955</td>\n",
              "      <td>...</td>\n",
              "      <td>0.069063</td>\n",
              "      <td>-0.037024</td>\n",
              "      <td>-0.043654</td>\n",
              "      <td>-0.100157</td>\n",
              "      <td>-0.145141</td>\n",
              "      <td>-0.170359</td>\n",
              "      <td>0.144143</td>\n",
              "      <td>-0.145218</td>\n",
              "      <td>-0.336380</td>\n",
              "      <td>-0.005551</td>\n",
              "      <td>0.303418</td>\n",
              "      <td>-0.031028</td>\n",
              "      <td>-0.029905</td>\n",
              "      <td>0.195296</td>\n",
              "      <td>-0.305234</td>\n",
              "      <td>0.015979</td>\n",
              "      <td>0.049964</td>\n",
              "      <td>0.195738</td>\n",
              "      <td>-0.159774</td>\n",
              "      <td>0.029840</td>\n",
              "      <td>-0.489543</td>\n",
              "      <td>0.222922</td>\n",
              "      <td>0.074329</td>\n",
              "      <td>0.288482</td>\n",
              "      <td>0.246239</td>\n",
              "      <td>0.008297</td>\n",
              "      <td>-0.150923</td>\n",
              "      <td>-0.076451</td>\n",
              "      <td>0.164616</td>\n",
              "      <td>-0.026882</td>\n",
              "      <td>-0.013977</td>\n",
              "      <td>0.011674</td>\n",
              "      <td>-0.020571</td>\n",
              "      <td>-0.029737</td>\n",
              "      <td>-0.004082</td>\n",
              "      <td>-0.149948</td>\n",
              "      <td>0.089315</td>\n",
              "      <td>-0.156816</td>\n",
              "      <td>-0.069571</td>\n",
              "      <td>-0.104728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[shall, late]</td>\n",
              "      <td>-0.090212</td>\n",
              "      <td>-0.071698</td>\n",
              "      <td>0.193818</td>\n",
              "      <td>-0.039499</td>\n",
              "      <td>-0.095114</td>\n",
              "      <td>0.027362</td>\n",
              "      <td>-0.101201</td>\n",
              "      <td>-0.071547</td>\n",
              "      <td>0.348967</td>\n",
              "      <td>0.008608</td>\n",
              "      <td>-0.111273</td>\n",
              "      <td>0.093129</td>\n",
              "      <td>0.060946</td>\n",
              "      <td>0.111183</td>\n",
              "      <td>0.022335</td>\n",
              "      <td>0.393584</td>\n",
              "      <td>0.043653</td>\n",
              "      <td>-0.030082</td>\n",
              "      <td>0.154541</td>\n",
              "      <td>-0.207357</td>\n",
              "      <td>-0.360797</td>\n",
              "      <td>0.070157</td>\n",
              "      <td>-0.007580</td>\n",
              "      <td>-0.199858</td>\n",
              "      <td>-0.099084</td>\n",
              "      <td>-0.048763</td>\n",
              "      <td>0.078787</td>\n",
              "      <td>0.168459</td>\n",
              "      <td>0.106232</td>\n",
              "      <td>0.064824</td>\n",
              "      <td>-0.103579</td>\n",
              "      <td>0.241880</td>\n",
              "      <td>-0.114393</td>\n",
              "      <td>0.124432</td>\n",
              "      <td>-0.068226</td>\n",
              "      <td>-0.049527</td>\n",
              "      <td>-0.080430</td>\n",
              "      <td>-0.165246</td>\n",
              "      <td>...</td>\n",
              "      <td>0.065275</td>\n",
              "      <td>-0.046945</td>\n",
              "      <td>-0.053838</td>\n",
              "      <td>-0.096982</td>\n",
              "      <td>-0.148763</td>\n",
              "      <td>-0.175016</td>\n",
              "      <td>0.139333</td>\n",
              "      <td>-0.143031</td>\n",
              "      <td>-0.327956</td>\n",
              "      <td>0.001885</td>\n",
              "      <td>0.306482</td>\n",
              "      <td>-0.026886</td>\n",
              "      <td>-0.021027</td>\n",
              "      <td>0.184712</td>\n",
              "      <td>-0.301301</td>\n",
              "      <td>0.020448</td>\n",
              "      <td>0.053619</td>\n",
              "      <td>0.194300</td>\n",
              "      <td>-0.153969</td>\n",
              "      <td>0.023008</td>\n",
              "      <td>-0.480453</td>\n",
              "      <td>0.224664</td>\n",
              "      <td>0.070125</td>\n",
              "      <td>0.285326</td>\n",
              "      <td>0.247936</td>\n",
              "      <td>0.003149</td>\n",
              "      <td>-0.161779</td>\n",
              "      <td>-0.081726</td>\n",
              "      <td>0.171440</td>\n",
              "      <td>-0.021733</td>\n",
              "      <td>-0.021447</td>\n",
              "      <td>0.003302</td>\n",
              "      <td>-0.025618</td>\n",
              "      <td>-0.036305</td>\n",
              "      <td>-0.004083</td>\n",
              "      <td>-0.138965</td>\n",
              "      <td>0.085015</td>\n",
              "      <td>-0.151768</td>\n",
              "      <td>-0.067145</td>\n",
              "      <td>-0.094273</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    author  ...        99\n",
              "0  Carroll  ... -0.095955\n",
              "1  Carroll  ... -0.082511\n",
              "2  Carroll  ... -0.095584\n",
              "3  Carroll  ... -0.104728\n",
              "4  Carroll  ... -0.094273\n",
              "\n",
              "[5 rows x 102 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J427hqp6oq2J",
        "colab_type": "text"
      },
      "source": [
        "**USE IN MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfG6RGVLoxBA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "18b7d420-4a32-4d8b-bc00-4a9165b05b23"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = sentences['author']\n",
        "X = np.array(sentences.drop(['text','author'], 1))\n",
        "\n",
        "# We split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
        "\n",
        "# Models\n",
        "lr = LogisticRegression()\n",
        "rfc = RandomForestClassifier()\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "rfc.fit(X_train, y_train)\n",
        "gbc.fit(X_train, y_train)\n",
        "\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train, y_train))\n",
        "print('\\nTest set score:', lr.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train, y_train))\n",
        "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train, y_train))\n",
        "print('\\nTest set score:', gbc.score(X_test, y_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.7367906066536204\n",
            "\n",
            "Test set score: 0.7451076320939335\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9944553163731246\n",
            "\n",
            "Test set score: 0.812133072407045\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.8956294846705806\n",
            "\n",
            "Test set score: 0.8082191780821918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyxL_LPIpDPZ",
        "colab_type": "text"
      },
      "source": [
        "**OLD SCORES:**\n",
        "\n",
        "----------------------Logistic Regression Scores----------------------\n",
        "Training set score: 0.7485540334855403\n",
        "\n",
        "Test set score: 0.7593607305936073\n",
        "\n",
        "----------------------Random Forest Scores----------------------\n",
        "Training set score: 0.9914764079147641\n",
        "\n",
        "Test set score: 0.7986301369863014\n",
        "\n",
        "----------------------Gradient Boosting Scores----------------------\n",
        "Training set score: 0.8867579908675799\n",
        "\n",
        "Test set score: 0.8082191780821918"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb8UOeoEpWr9",
        "colab_type": "text"
      },
      "source": [
        "**OVERALL ABOUT THE SAME**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whnTH2R_pduK",
        "colab_type": "text"
      },
      "source": [
        "LOGITIC REGRESSION TEST SCORE IMPROVED\n",
        "\n",
        "RANDOM FOREST TEST WORSE\n",
        "\n",
        "GRADIENT BOOST -- IDENTICAL TEST SCORE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLzm7cc-qfB1",
        "colab_type": "text"
      },
      "source": [
        "**///////////////////////////////////////////////////////////////////**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo2P-9H2qi5M",
        "colab_type": "text"
      },
      "source": [
        "**TRY ONCE MORE WITH A FEW CHANGES**\n",
        "\n",
        "INCREASE WINDOW \n",
        "\n",
        "INCREASE SIZE\n",
        "\n",
        "BACK TO HIERARCHICAL\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leA1kNBaq2t9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train word2vec on the the sentences\n",
        "model = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=7,\n",
        "    sg=1,\n",
        "    sample=1e-3,\n",
        "    size=150,\n",
        "    hs=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTJEy1WYrHNe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "043d841d-6534-42b7-91e0-7078e21f5360"
      },
      "source": [
        "print(model.most_similar(positive=['lady', 'man'], negative=['woman'], topn=5))\n",
        "print(model.doesnt_match(\"dad dinner mom aunt uncle\".split()))\n",
        "print(model.similarity('woman', 'man'))\n",
        "print(model.similarity('horse', 'cat'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('drawing', 0.941796600818634), ('state', 0.9320310950279236), ('gallant', 0.9194947481155396), ('remain', 0.9146274924278259), ('Harvilles', 0.9136101007461548)]\n",
            "dinner\n",
            "0.9044591\n",
            "0.55610454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srx2Lr4HrZ2i",
        "colab_type": "text"
      },
      "source": [
        "**CREATE DATASET WITH 100 NUMERICAL FEATURES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rG4ppfcrf-y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "bb7957d0-cbeb-4a09-8db2-69fc04b93fc2"
      },
      "source": [
        "word2vec_arr = np.zeros((sentences.shape[0],150))\n",
        "\n",
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "    word2vec_arr[i,:] = np.mean([model[lemma] for lemma in sentence], axis=0)\n",
        "\n",
        "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
        "sentences = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
        "sentences.dropna(inplace=True)\n",
        "\n",
        "sentences.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>...</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[Alice, begin, tired, sit, sister, bank, have,...</td>\n",
              "      <td>-0.084378</td>\n",
              "      <td>0.018499</td>\n",
              "      <td>0.260216</td>\n",
              "      <td>0.046439</td>\n",
              "      <td>-0.221608</td>\n",
              "      <td>0.146420</td>\n",
              "      <td>0.085370</td>\n",
              "      <td>-0.027138</td>\n",
              "      <td>0.134531</td>\n",
              "      <td>-0.035061</td>\n",
              "      <td>0.029242</td>\n",
              "      <td>-0.004889</td>\n",
              "      <td>0.137644</td>\n",
              "      <td>0.224693</td>\n",
              "      <td>0.063269</td>\n",
              "      <td>0.220398</td>\n",
              "      <td>0.095539</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>0.128979</td>\n",
              "      <td>-0.100360</td>\n",
              "      <td>-0.069317</td>\n",
              "      <td>0.061253</td>\n",
              "      <td>0.054233</td>\n",
              "      <td>-0.220689</td>\n",
              "      <td>-0.097462</td>\n",
              "      <td>0.006515</td>\n",
              "      <td>-0.018772</td>\n",
              "      <td>0.130268</td>\n",
              "      <td>-0.031855</td>\n",
              "      <td>0.203759</td>\n",
              "      <td>-0.111214</td>\n",
              "      <td>0.193110</td>\n",
              "      <td>-0.128819</td>\n",
              "      <td>-0.001490</td>\n",
              "      <td>-0.059513</td>\n",
              "      <td>0.059552</td>\n",
              "      <td>-0.084287</td>\n",
              "      <td>-0.223194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006753</td>\n",
              "      <td>0.091144</td>\n",
              "      <td>-0.224683</td>\n",
              "      <td>0.108219</td>\n",
              "      <td>0.180380</td>\n",
              "      <td>-0.008803</td>\n",
              "      <td>0.123890</td>\n",
              "      <td>-0.188165</td>\n",
              "      <td>-0.156035</td>\n",
              "      <td>0.079360</td>\n",
              "      <td>0.062038</td>\n",
              "      <td>0.129331</td>\n",
              "      <td>-0.149438</td>\n",
              "      <td>-0.059564</td>\n",
              "      <td>-0.014594</td>\n",
              "      <td>-0.175428</td>\n",
              "      <td>0.043180</td>\n",
              "      <td>0.112494</td>\n",
              "      <td>0.000294</td>\n",
              "      <td>-0.296572</td>\n",
              "      <td>-0.012595</td>\n",
              "      <td>-0.111965</td>\n",
              "      <td>0.055118</td>\n",
              "      <td>-0.094838</td>\n",
              "      <td>0.202652</td>\n",
              "      <td>0.126110</td>\n",
              "      <td>0.083071</td>\n",
              "      <td>0.154961</td>\n",
              "      <td>-0.123595</td>\n",
              "      <td>-0.040393</td>\n",
              "      <td>-0.274358</td>\n",
              "      <td>0.014741</td>\n",
              "      <td>0.187632</td>\n",
              "      <td>0.108080</td>\n",
              "      <td>0.156315</td>\n",
              "      <td>-0.066466</td>\n",
              "      <td>0.022220</td>\n",
              "      <td>0.040301</td>\n",
              "      <td>0.129662</td>\n",
              "      <td>-0.171606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[consider, mind, hot, day, feel, sleepy, stupi...</td>\n",
              "      <td>-0.028472</td>\n",
              "      <td>0.028775</td>\n",
              "      <td>0.262261</td>\n",
              "      <td>0.072930</td>\n",
              "      <td>-0.180799</td>\n",
              "      <td>0.096593</td>\n",
              "      <td>0.080446</td>\n",
              "      <td>-0.003522</td>\n",
              "      <td>0.092186</td>\n",
              "      <td>-0.034481</td>\n",
              "      <td>0.002959</td>\n",
              "      <td>-0.011663</td>\n",
              "      <td>0.120195</td>\n",
              "      <td>0.201413</td>\n",
              "      <td>0.080199</td>\n",
              "      <td>0.207363</td>\n",
              "      <td>0.097010</td>\n",
              "      <td>-0.010622</td>\n",
              "      <td>0.132023</td>\n",
              "      <td>-0.074780</td>\n",
              "      <td>-0.064657</td>\n",
              "      <td>0.066582</td>\n",
              "      <td>0.067479</td>\n",
              "      <td>-0.197422</td>\n",
              "      <td>-0.080487</td>\n",
              "      <td>0.020710</td>\n",
              "      <td>0.017442</td>\n",
              "      <td>0.124838</td>\n",
              "      <td>-0.045598</td>\n",
              "      <td>0.193651</td>\n",
              "      <td>-0.101412</td>\n",
              "      <td>0.185312</td>\n",
              "      <td>-0.121770</td>\n",
              "      <td>-0.012521</td>\n",
              "      <td>-0.048765</td>\n",
              "      <td>0.007971</td>\n",
              "      <td>-0.082453</td>\n",
              "      <td>-0.178780</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002039</td>\n",
              "      <td>0.090419</td>\n",
              "      <td>-0.223923</td>\n",
              "      <td>0.080785</td>\n",
              "      <td>0.196260</td>\n",
              "      <td>0.011093</td>\n",
              "      <td>0.153335</td>\n",
              "      <td>-0.165216</td>\n",
              "      <td>-0.121276</td>\n",
              "      <td>0.046865</td>\n",
              "      <td>0.074955</td>\n",
              "      <td>0.136277</td>\n",
              "      <td>-0.134411</td>\n",
              "      <td>-0.039464</td>\n",
              "      <td>-0.008681</td>\n",
              "      <td>-0.162899</td>\n",
              "      <td>0.011784</td>\n",
              "      <td>0.133724</td>\n",
              "      <td>-0.009869</td>\n",
              "      <td>-0.270960</td>\n",
              "      <td>0.007105</td>\n",
              "      <td>-0.128114</td>\n",
              "      <td>0.017706</td>\n",
              "      <td>-0.058690</td>\n",
              "      <td>0.190532</td>\n",
              "      <td>0.146836</td>\n",
              "      <td>0.099117</td>\n",
              "      <td>0.118884</td>\n",
              "      <td>-0.137521</td>\n",
              "      <td>-0.068001</td>\n",
              "      <td>-0.215993</td>\n",
              "      <td>-0.015412</td>\n",
              "      <td>0.128415</td>\n",
              "      <td>0.091065</td>\n",
              "      <td>0.106460</td>\n",
              "      <td>-0.051644</td>\n",
              "      <td>0.018346</td>\n",
              "      <td>0.023013</td>\n",
              "      <td>0.119060</td>\n",
              "      <td>-0.125782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[remarkable, Alice, think, way, hear, Rabbit, ...</td>\n",
              "      <td>-0.060983</td>\n",
              "      <td>0.032306</td>\n",
              "      <td>0.260433</td>\n",
              "      <td>0.126023</td>\n",
              "      <td>-0.193101</td>\n",
              "      <td>0.115526</td>\n",
              "      <td>0.076531</td>\n",
              "      <td>-0.001532</td>\n",
              "      <td>0.092028</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.017373</td>\n",
              "      <td>-0.018527</td>\n",
              "      <td>0.145436</td>\n",
              "      <td>0.248440</td>\n",
              "      <td>0.077122</td>\n",
              "      <td>0.213051</td>\n",
              "      <td>0.097073</td>\n",
              "      <td>0.000576</td>\n",
              "      <td>0.107587</td>\n",
              "      <td>-0.067797</td>\n",
              "      <td>-0.053518</td>\n",
              "      <td>0.029185</td>\n",
              "      <td>0.083014</td>\n",
              "      <td>-0.218593</td>\n",
              "      <td>-0.079607</td>\n",
              "      <td>0.001452</td>\n",
              "      <td>0.008350</td>\n",
              "      <td>0.171610</td>\n",
              "      <td>-0.052628</td>\n",
              "      <td>0.173561</td>\n",
              "      <td>-0.141169</td>\n",
              "      <td>0.206908</td>\n",
              "      <td>-0.187256</td>\n",
              "      <td>-0.017524</td>\n",
              "      <td>-0.028762</td>\n",
              "      <td>0.088768</td>\n",
              "      <td>-0.100092</td>\n",
              "      <td>-0.228569</td>\n",
              "      <td>...</td>\n",
              "      <td>0.108609</td>\n",
              "      <td>0.097236</td>\n",
              "      <td>-0.242244</td>\n",
              "      <td>0.103491</td>\n",
              "      <td>0.198991</td>\n",
              "      <td>-0.007100</td>\n",
              "      <td>0.182838</td>\n",
              "      <td>-0.221555</td>\n",
              "      <td>-0.103729</td>\n",
              "      <td>0.056800</td>\n",
              "      <td>0.095354</td>\n",
              "      <td>0.177517</td>\n",
              "      <td>-0.172848</td>\n",
              "      <td>-0.074063</td>\n",
              "      <td>0.015420</td>\n",
              "      <td>-0.119620</td>\n",
              "      <td>-0.005409</td>\n",
              "      <td>0.151671</td>\n",
              "      <td>0.083081</td>\n",
              "      <td>-0.386724</td>\n",
              "      <td>-0.031066</td>\n",
              "      <td>-0.181881</td>\n",
              "      <td>0.018626</td>\n",
              "      <td>-0.079580</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>0.110591</td>\n",
              "      <td>0.104018</td>\n",
              "      <td>0.150730</td>\n",
              "      <td>-0.129412</td>\n",
              "      <td>-0.092924</td>\n",
              "      <td>-0.271747</td>\n",
              "      <td>-0.023699</td>\n",
              "      <td>0.183407</td>\n",
              "      <td>0.084136</td>\n",
              "      <td>0.112303</td>\n",
              "      <td>-0.035835</td>\n",
              "      <td>-0.013410</td>\n",
              "      <td>0.053706</td>\n",
              "      <td>0.130780</td>\n",
              "      <td>-0.130038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[oh, dear]</td>\n",
              "      <td>-0.116564</td>\n",
              "      <td>0.104293</td>\n",
              "      <td>0.158561</td>\n",
              "      <td>0.055310</td>\n",
              "      <td>-0.157164</td>\n",
              "      <td>0.085965</td>\n",
              "      <td>-0.000582</td>\n",
              "      <td>-0.058927</td>\n",
              "      <td>0.095363</td>\n",
              "      <td>-0.006801</td>\n",
              "      <td>0.034306</td>\n",
              "      <td>-0.042300</td>\n",
              "      <td>0.187039</td>\n",
              "      <td>0.215381</td>\n",
              "      <td>0.050706</td>\n",
              "      <td>0.230333</td>\n",
              "      <td>0.078028</td>\n",
              "      <td>0.066463</td>\n",
              "      <td>0.095453</td>\n",
              "      <td>-0.040032</td>\n",
              "      <td>-0.073373</td>\n",
              "      <td>0.048078</td>\n",
              "      <td>0.045447</td>\n",
              "      <td>-0.180907</td>\n",
              "      <td>-0.107310</td>\n",
              "      <td>-0.020279</td>\n",
              "      <td>-0.061278</td>\n",
              "      <td>0.095666</td>\n",
              "      <td>-0.005257</td>\n",
              "      <td>0.133415</td>\n",
              "      <td>-0.072658</td>\n",
              "      <td>0.142433</td>\n",
              "      <td>-0.157838</td>\n",
              "      <td>0.002671</td>\n",
              "      <td>-0.032974</td>\n",
              "      <td>0.177357</td>\n",
              "      <td>-0.052596</td>\n",
              "      <td>-0.200444</td>\n",
              "      <td>...</td>\n",
              "      <td>0.068565</td>\n",
              "      <td>0.101806</td>\n",
              "      <td>-0.195483</td>\n",
              "      <td>0.136282</td>\n",
              "      <td>0.145833</td>\n",
              "      <td>-0.021141</td>\n",
              "      <td>0.149250</td>\n",
              "      <td>-0.254835</td>\n",
              "      <td>-0.106250</td>\n",
              "      <td>0.055685</td>\n",
              "      <td>0.025446</td>\n",
              "      <td>0.109745</td>\n",
              "      <td>-0.160365</td>\n",
              "      <td>-0.054904</td>\n",
              "      <td>-0.003928</td>\n",
              "      <td>-0.147603</td>\n",
              "      <td>-0.015027</td>\n",
              "      <td>0.057542</td>\n",
              "      <td>0.140618</td>\n",
              "      <td>-0.338251</td>\n",
              "      <td>-0.005533</td>\n",
              "      <td>-0.111485</td>\n",
              "      <td>0.119561</td>\n",
              "      <td>-0.067606</td>\n",
              "      <td>0.240044</td>\n",
              "      <td>0.074989</td>\n",
              "      <td>0.076062</td>\n",
              "      <td>0.143343</td>\n",
              "      <td>-0.104508</td>\n",
              "      <td>-0.064450</td>\n",
              "      <td>-0.292905</td>\n",
              "      <td>0.010636</td>\n",
              "      <td>0.189469</td>\n",
              "      <td>0.120415</td>\n",
              "      <td>0.133286</td>\n",
              "      <td>-0.063201</td>\n",
              "      <td>-0.067698</td>\n",
              "      <td>0.044902</td>\n",
              "      <td>0.112977</td>\n",
              "      <td>-0.170898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[shall, late]</td>\n",
              "      <td>-0.111882</td>\n",
              "      <td>0.044768</td>\n",
              "      <td>0.222980</td>\n",
              "      <td>0.069981</td>\n",
              "      <td>-0.172694</td>\n",
              "      <td>0.114725</td>\n",
              "      <td>0.000695</td>\n",
              "      <td>-0.051019</td>\n",
              "      <td>0.135941</td>\n",
              "      <td>-0.038545</td>\n",
              "      <td>-0.001035</td>\n",
              "      <td>-0.007677</td>\n",
              "      <td>0.139189</td>\n",
              "      <td>0.210357</td>\n",
              "      <td>0.089719</td>\n",
              "      <td>0.274147</td>\n",
              "      <td>0.079383</td>\n",
              "      <td>-0.021677</td>\n",
              "      <td>0.135527</td>\n",
              "      <td>-0.067735</td>\n",
              "      <td>-0.087572</td>\n",
              "      <td>0.104314</td>\n",
              "      <td>0.029106</td>\n",
              "      <td>-0.194809</td>\n",
              "      <td>-0.107184</td>\n",
              "      <td>0.003298</td>\n",
              "      <td>0.005820</td>\n",
              "      <td>0.063873</td>\n",
              "      <td>-0.029725</td>\n",
              "      <td>0.201526</td>\n",
              "      <td>-0.102864</td>\n",
              "      <td>0.166580</td>\n",
              "      <td>-0.116645</td>\n",
              "      <td>0.044853</td>\n",
              "      <td>-0.018541</td>\n",
              "      <td>0.037376</td>\n",
              "      <td>-0.074647</td>\n",
              "      <td>-0.178170</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.068370</td>\n",
              "      <td>0.072020</td>\n",
              "      <td>-0.200865</td>\n",
              "      <td>0.101388</td>\n",
              "      <td>0.109450</td>\n",
              "      <td>0.006601</td>\n",
              "      <td>0.144892</td>\n",
              "      <td>-0.183364</td>\n",
              "      <td>-0.162378</td>\n",
              "      <td>0.075774</td>\n",
              "      <td>0.016419</td>\n",
              "      <td>0.140988</td>\n",
              "      <td>-0.118580</td>\n",
              "      <td>-0.041544</td>\n",
              "      <td>0.013988</td>\n",
              "      <td>-0.190777</td>\n",
              "      <td>-0.059691</td>\n",
              "      <td>0.109710</td>\n",
              "      <td>0.009864</td>\n",
              "      <td>-0.256601</td>\n",
              "      <td>0.004386</td>\n",
              "      <td>-0.072144</td>\n",
              "      <td>0.039087</td>\n",
              "      <td>-0.030055</td>\n",
              "      <td>0.268397</td>\n",
              "      <td>0.043660</td>\n",
              "      <td>0.072215</td>\n",
              "      <td>0.122976</td>\n",
              "      <td>-0.095808</td>\n",
              "      <td>-0.026498</td>\n",
              "      <td>-0.231246</td>\n",
              "      <td>0.001003</td>\n",
              "      <td>0.176714</td>\n",
              "      <td>0.089559</td>\n",
              "      <td>0.116098</td>\n",
              "      <td>-0.100573</td>\n",
              "      <td>0.039992</td>\n",
              "      <td>0.007634</td>\n",
              "      <td>0.120263</td>\n",
              "      <td>-0.150806</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 152 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    author  ...       149\n",
              "0  Carroll  ... -0.171606\n",
              "1  Carroll  ... -0.125782\n",
              "2  Carroll  ... -0.130038\n",
              "3  Carroll  ... -0.170898\n",
              "4  Carroll  ... -0.150806\n",
              "\n",
              "[5 rows x 152 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImET_GdQrxxh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "c870c4e4-7458-4167-97c1-81374ef4d4b9"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = sentences['author']\n",
        "X = np.array(sentences.drop(['text','author'], 1))\n",
        "\n",
        "# We split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
        "\n",
        "# Models\n",
        "lr = LogisticRegression()\n",
        "rfc = RandomForestClassifier()\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "rfc.fit(X_train, y_train)\n",
        "gbc.fit(X_train, y_train)\n",
        "\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train, y_train))\n",
        "print('\\nTest set score:', lr.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train, y_train))\n",
        "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train, y_train))\n",
        "print('\\nTest set score:', gbc.score(X_test, y_test))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.8338590956887487\n",
            "\n",
            "Test set score: 0.8170347003154574\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9922888187872415\n",
            "\n",
            "Test set score: 0.8338590956887487\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.9120224325271644\n",
            "\n",
            "Test set score: 0.8375394321766562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcq7MzxKr-Ja",
        "colab_type": "text"
      },
      "source": [
        "OLD SCORES:\n",
        "\n",
        "----------------------Logistic Regression Scores---------------------- Training set score: 0.7485540334855403\n",
        "\n",
        "Test set score: 0.7593607305936073\n",
        "\n",
        "----------------------Random Forest Scores---------------------- Training set score: 0.9914764079147641\n",
        "\n",
        "Test set score: 0.7986301369863014\n",
        "\n",
        "----------------------Gradient Boosting Scores---------------------- Training set score: 0.8867579908675799\n",
        "\n",
        "Test set score: 0.8082191780821918"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEtcb2BZsC0A",
        "colab_type": "text"
      },
      "source": [
        "**BETTER FOR LOGISTIC REGRESSION**\n",
        "\n",
        "**LOWER FOR RF and GB**"
      ]
    }
  ]
}