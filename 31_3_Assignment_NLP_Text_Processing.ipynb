{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "31.3 Assignment NLP Text Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNp45KMafQg2UbXJCgRadcH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexBB999/NLP/blob/master/31_3_Assignment_NLP_Text_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfDLL-w17nFr",
        "colab_type": "code",
        "outputId": "ce7da321-cbcd-4429-9667-8f45b041f502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5ea5czwfh-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh7NyvGN8LDQ",
        "colab_type": "code",
        "outputId": "cd091005-f6d9-43fd-dc19-2beefe5f2e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "! pip install spacy\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.38.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPt5TrbY8Sz_",
        "colab_type": "code",
        "outputId": "1ef4f17c-0be1-404a-b967-09e29069540d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "! python -m spacy download en\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL_jkkUY8g8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "nlp = spacy.load('en',disable=['parser','ner'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxt63Qto8stz",
        "colab_type": "code",
        "outputId": "3e502669-5386-44bd-b8aa-84839aea36e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Launch the installer to download Gutenberg corpus\n",
        "nltk.download(\"gutenberg\")\n",
        "\n",
        "# Download the English models of SpaCy\n",
        "#!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POJ1KS0k81E9",
        "colab_type": "code",
        "outputId": "5fde4802-0880-4d64-a9af-c7948dc9e613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "# import the data we just downloaded\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# grab and process the raw data\n",
        "print(gutenberg.fileids())\n",
        "\n",
        "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
        "alice = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# print the first 100 characters of Alice\n",
        "print('\\nRaw:\\n', alice[0:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "\n",
            "Raw:\n",
            " [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
            "\n",
            "CHAPTER I. Down the Rabbit-Hole\n",
            "\n",
            "Alice was\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVpMq-D49Fzq",
        "colab_type": "text"
      },
      "source": [
        "###**Basic text cleaning\n",
        "When modifying text data, using regular expressions is a common practice **bold text**.\n",
        "\n",
        " We're also going to use regular expressions (**specifically re.sub(), short for \"substitute\")** to identify and remove substrings we don't want. \n",
        " \n",
        " \n",
        " Specifically, **we'll match those substrings with a regular expression and substitute in an empty string for them.**\n",
        "\n",
        " If you want more information the Python Regular Expression HOWTO is an accessible starting point and reference, and RegExr is a useful tool for visualizing and tinkering with regular expressions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVZdRlHs-cEU",
        "colab_type": "text"
      },
      "source": [
        "We'll start our cleaning by removing the title.\n",
        "\n",
        " **We'll match all text between square brackets and replace it with an empty string**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MXMj2XY9lXX",
        "colab_type": "code",
        "outputId": "44ad9f97-6f2e-4351-8e1e-d2dbb0d69e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# this pattern matches all text between square brackets\n",
        "pattern = \"[\\[].*?[\\]]\"\n",
        "persuasion = re.sub(pattern, \"\", persuasion)\n",
        "alice = re.sub(pattern, \"\", alice)\n",
        "\n",
        "# print the first 100 characters of Alice again\n",
        "print(\"Title removed:\", alice[0:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title removed: \n",
            "\n",
            "CHAPTER I. Down the Rabbit-Hole\n",
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xok_RGFj-yye",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll remove the chapter headings like CHAPTER I.\n",
        " Note that two novels have different styles of chapter headings.\n",
        "\n",
        "So, we deal with each novel one by one.\n",
        "\n",
        "This is **quite usual** in cleaning text data.\n",
        "\n",
        "As we said before, **all texts have their own peculiarities and cleaning them requires you to know those peculiaritie**s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMaQqWyN_BTW",
        "colab_type": "code",
        "outputId": "f6cb708f-c456-49cd-8d19-9a0769b6a2a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# now we'll match and remove chapter headings\n",
        "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
        "alice = re.sub(r'CHAPTER .*', '', alice)\n",
        "\n",
        "# ok, what's it look like now?\n",
        "print('Chapter headings removed:', alice[0:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chapter headings removed: \n",
            "\n",
            "\n",
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF08o2lT_HBG",
        "colab_type": "text"
      },
      "source": [
        "If you were to read the two novels, you'd notice that there are **a lot of \"new line\" characters and other types of extra whitespaces.**\n",
        "\n",
        " So, **we need to clean them up**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JnXngdC_OIM",
        "colab_type": "code",
        "outputId": "2b4a3fc3-335d-454c-eabd-511ab8625da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# remove newlines and other extra whitespace by splitting and rejoining\n",
        "persuasion = ' '.join(persuasion.split())\n",
        "alice = ' '.join(alice.split())\n",
        "\n",
        "# all done with cleanup? let's see how it looks.\n",
        "print('Extra whitespace removed:', alice[0:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extra whitespace removed: CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jK9yPmx_drW",
        "colab_type": "text"
      },
      "source": [
        "Much of the things you saw as data cleaning so far were just a demonstration of what kind of problems you may encounter in a corpus. You can imagine a lot more than what we showed here. For example, if we were to work on a social media corpus, then we most likely would encounter with many emojis and abbreviations. So, dealing with them would also be a major problem in the data cleaning phase.\n",
        "\n",
        "**Hence, you should always be careful about what kind of corpus you have and what types of problems may occur in the text**.\n",
        "\n",
        "Since our text started to look okay, the next step is to tokenize our texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnDe1WcW_gbE",
        "colab_type": "text"
      },
      "source": [
        "##**Tokenization**\n",
        "\n",
        "As you recall from the previous checkpoint, **each individual meaningful piece from a text is called a token**,\n",
        "\n",
        " and **the process of breaking up the text into these pieces is called tokenization**.\n",
        " \n",
        " Tokenization is an important step in text preprocessing, because most of the time we generate the numerical representations of our texts from these tokens. Hence, breaking up the text into tokens correctly is a crucial step for the success of the next steps of any data science workflow.\n",
        "\n",
        "**Tokens are generally words and punctuation**. \n",
        "\n",
        "In some NLP applications, you may see that people remove the punctuations from the text as if they are stopwords. There's no a single correct way of handling the punctuations and it's usually a matter of experimentation to determine the best way.\n",
        "\n",
        " I**n the following, we'll keep punctuations in our documents as we'll make use of them when separating our text into sentences**.\n",
        " \n",
        "  **However, when we analyze our data, we check for them and don't include them in our analysi**s as you'll see shortly.\n",
        "\n",
        "Let's go ahead and use spaCy to parse our novels into tokens.\n",
        "\n",
        " **When we call spaCy on the novel it will immediately and automatically parse it, tokenizing the string by breaking it into words and punctuation (and many other things we will explore)**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKxi2Y7X_wrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# all the processing work is done below, so it may take a while\n",
        "alice_doc = nlp(alice)\n",
        "persuasion_doc = nlp(persuasion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wEv3Tc3AeqI",
        "colab_type": "text"
      },
      "source": [
        "All our parsed documents are now stored in two variables we defined.\n",
        "\n",
        " SpaCy did a lot of good things when parsing the documents. \n",
        "\n",
        "Let's see what we have after the parsing happened:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRu5LddmAgFm",
        "colab_type": "code",
        "outputId": "cf59e4f3-bce4-4a4d-b8bd-3b8c4ca6daa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# let's explore the objects we've built.\n",
        "print(\"The alice_doc object is a {} object.\".format(type(alice_doc)))\n",
        "print(\"It is {} tokens long\".format(len(alice_doc)))\n",
        "print(\"The first three tokens are '{}'\".format(alice_doc[:3]))\n",
        "print(\"The type of each token is {}\".format(type(alice_doc[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The alice_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
            "It is 34495 tokens long\n",
            "The first three tokens are 'CHAPTER I. Down'\n",
            "The type of each token is <class 'spacy.tokens.token.Token'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxWlW2tMA3td",
        "colab_type": "text"
      },
      "source": [
        "We see from introspecting the spaCy objects above that we're playing around with **doc and token objects.** \n",
        "\n",
        "**These are the types that are defined by SpaCy**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH5Hk_IwBCSJ",
        "colab_type": "text"
      },
      "source": [
        "##**Removing stopwords**\n",
        "\n",
        "One of the important steps of text preprocessing is to remove the stopwords from the dataset.\n",
        "\n",
        "This is because they occur a lot in the text and most of the time they convey little meaning. So removing them benefits twice:\n",
        "\n",
        "**We get rid of the noisy data**.\n",
        "\n",
        "The size of the text diminishes and hence the computation time shortens.\n",
        "\n",
        "Removing stopwords with SpaCy is quite easy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j45qghpoBKKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alice_without_stopwords = [token for token in alice_doc if not token.is_stop]\n",
        "persuasion_without_stopwords = [token for token in persuasion_doc if not token.is_stop]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07d6X8-1r0nc",
        "colab_type": "code",
        "outputId": "dd5bc8b5-a96c-475a-e93d-3ea6645db33e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(alice_without_stopwords)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emQo6R_GBjtb",
        "colab_type": "text"
      },
      "source": [
        "As you can see, **we just iterated over the tokens that are already made available by SpaCy** during the parsing of the documents and exclude the token from the list if it's a stopword.\n",
        "\n",
        "**Now, we store our tokens in two lists that are free of stopwords**. \n",
        "\n",
        "Let's stop text processing a little bit and look at how frequent each token is in our corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub_8wLSuB0cc",
        "colab_type": "code",
        "outputId": "665652e6-39ed-416a-9d5e-ff373e5d496a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# utility function to calculate how frequently words appear in the text\n",
        "def word_frequencies(text):\n",
        "    \n",
        "    # build a list of words\n",
        "    # strip out punctuation\n",
        "    words = []\n",
        "    for token in text:\n",
        "        if not token.is_punct:\n",
        "            words.append(token.text)\n",
        "            \n",
        "    # build and return a Counter object containing word counts\n",
        "    return Counter(words)\n",
        "\n",
        "# instantiate our list of most common words.\n",
        "alice_word_freq = word_frequencies(alice_without_stopwords).most_common(10)\n",
        "persuasion_word_freq = word_frequencies(persuasion_without_stopwords).most_common(10)\n",
        "print('\\nAlice:', alice_word_freq)\n",
        "print('Persuasion:', persuasion_word_freq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alice: [('said', 453), ('Alice', 394), ('little', 124), ('like', 84), ('went', 83), ('know', 83), ('thought', 74), ('Queen', 74), ('time', 68), ('King', 61)]\n",
            "Persuasion: [('Anne', 496), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('Mr', 254), ('Wentworth', 217), ('Lady', 191), ('good', 181), ('little', 175), ('Charles', 166)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKii08gvB8-f",
        "colab_type": "text"
      },
      "source": [
        "Just take a moment and think about the 10 most common words in each novel.\n",
        "\n",
        " Do you see some differences that make sense to you"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2GLy4DfCHw1",
        "colab_type": "text"
      },
      "source": [
        "##**Lemmatization**\n",
        "\n",
        "So far, we've tokenized our texts looked at whether certain words are present and how frequently they appear. \n",
        "\n",
        "**We can process these words further to remove a little more noise from our data**.\n",
        "\n",
        " Consider the words \"think\", \"thought\", and \"thinking\". They're related. They all share the same root word: the verb \"think\". Most of the times, we want to focus on the fact that the act of thinking comes up a lot in data, and not have that information split across all the different forms of \"think\".\n",
        "\n",
        "To focus in like this, **we can reduce each word to its root that is to lemma and do our counts again**.\n",
        "\n",
        " **This time, we're building a count of concepts rather than just words**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRCxi_4SCeky",
        "colab_type": "code",
        "outputId": "f1dc2391-5e78-443f-edb1-b0de556d9bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# utility function to calculate how frequently lemas appear in the text\n",
        "def lemma_frequencies(text):\n",
        "    \n",
        "    # build a list of lemas\n",
        "    # strip out punctuation\n",
        "    lemmas = []\n",
        "    for token in text:\n",
        "        if not token.is_punct:\n",
        "            lemmas.append(token.lemma_)\n",
        "            \n",
        "    # build and return a Counter object containing lemma counts\n",
        "    return Counter(lemmas)\n",
        "\n",
        "# instantiate our list of most common lemmas\n",
        "alice_lemma_freq = lemma_frequencies(alice_without_stopwords).most_common(10)\n",
        "persuasion_lemma_freq = lemma_frequencies(persuasion_without_stopwords).most_common(10)\n",
        "print('\\nAlice:', alice_lemma_freq)\n",
        "print('Persuasion:', persuasion_lemma_freq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alice: [('say', 476), ('Alice', 394), ('think', 130), ('go', 130), ('little', 125), ('look', 105), ('know', 103), ('come', 96), ('like', 92), ('begin', 91)]\n",
            "Persuasion: [('Anne', 496), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('think', 258), ('Mr', 254), ('know', 252), ('good', 222), ('Wentworth', 215), ('Lady', 191)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulJ6uvNkC4ta",
        "colab_type": "text"
      },
      "source": [
        "**As you can realize, the top ten list changed**.\n",
        "\n",
        " You can try to print more number of top lemmas and catch meaningful differences between the two novels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P50jHc_1DASr",
        "colab_type": "text"
      },
      "source": [
        "**Alternatively, we can identify the lemmas common to one text but not the other**.\n",
        "\n",
        " This may help us in understanding the differences between the two novels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2D5wsCbDG0q",
        "colab_type": "code",
        "outputId": "b4df3480-3c7a-41a7-e5de-4d74fb323b1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "alice_lemma_common = [pair[0] for pair in alice_lemma_freq]\n",
        "persuasion_lemma_common = [pair[0] for pair in persuasion_lemma_freq]\n",
        "print('Unique to Alice:', set(alice_lemma_common) - set(persuasion_lemma_common))\n",
        "print('Unique to Persuasion:', set(persuasion_lemma_common) - set(alice_lemma_common))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique to Alice: {'Alice', 'say', 'begin', 'go', 'little', 'like', 'come', 'look'}\n",
            "Unique to Persuasion: {'Elliot', 'Mrs', 'good', 'Wentworth', 'Anne', 'Captain', 'Lady', 'Mr'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyTbjhQfhrUH",
        "colab_type": "text"
      },
      "source": [
        "These are examples of how you can do data exploration on text data. When it comes to text data, the limit is sky! So, use your imagination and find out more creative ways of analyzing the two novels based on the lemmas they have.\n",
        "\n",
        "We'll not go into the details but some syntactical properties can also help in this analysis. \n",
        "If you notice, the most frequent lemmas include person names. For the purpose of our analysis, we may need to eliminate them from the lists. \n",
        "\n",
        "In order to do this, we can derive the named entities in the texts and SpaCy has already derived named entities in the texts during parsing.\n",
        "\n",
        "If you like, you can go ahead and inspect the named entities.\n",
        "\n",
        "Note: We lemmatized our tokens to treat words with similar meanings as if they are the same. \n",
        "\n",
        "Apart from looking at lemmas, we could also perform a similar analysis by pulling out prefixes (token.prefix_) or suffixes (token.suffix_) from the tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo0EbOTPiHw_",
        "colab_type": "text"
      },
      "source": [
        "##**Sentences**\n",
        "\n",
        "Before closing this checkpoint, we want to mention about how to determine the sentences in a corpus.\n",
        "\n",
        "Beyond individual words, text can also be considered at the level of sentences. Using punctuation cues, we can split up text into sentences.\n",
        "\n",
        "Each sentence can then be summarized by, for example, using sentiment analysis to categorize sentences as having positive or negative sentiment.\n",
        "\n",
        " We may also be interested in how long sentences tend to be, and how many unique words make up a sentence.\n",
        "\n",
        " The sentence also provides context for the individual words, allowing us to draw even more information from each word.\n",
        "\n",
        "We get a lot of automatic sentence-level information from spaCy. The **doc.sents** property will give us each sentence as a span object. \n",
        "\n",
        "Let's look at some of that:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyufEvLsiNLp",
        "colab_type": "code",
        "outputId": "9e19b0c4-aa32-4563-f5ea-6de52494c26b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# initial exploration of sentences\n",
        "\n",
        "sentences = list(alice_doc.sents)\n",
        "print(\"Alice in Wonderland has {} sentences.\".format(len(sentences)))\n",
        "\n",
        "example_sentence = sentences[2]\n",
        "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alice in Wonderland has 1624 sentences.\n",
            "Here is an example: \n",
            "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlIu4DaUizJy",
        "colab_type": "text"
      },
      "source": [
        " **look at some metrics around this sentence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX41DExyishc",
        "colab_type": "code",
        "outputId": "289b1e86-b087-4791-c208-938a51e3da1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_words = [token for token in example_sentence if not token.is_punct]\n",
        "unique_words = set([token.text for token in example_words])\n",
        "\n",
        "print((\"There are {} words in this sentence, and {} of them are\"\n",
        "       \" unique.\").format(len(example_words), len(unique_words)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 56 words in this sentence, and 46 of them are unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AgHc3R2jDmV",
        "colab_type": "text"
      },
      "source": [
        "As we can see, sentence-level analysis can also be helpful in the data exploration phase.\n",
        "\n",
        "This is all about data cleaning and text preprocessing for now. It's your turn to complete the assignments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqMfyddPjPcp",
        "colab_type": "text"
      },
      "source": [
        "##**Assignments**\n",
        "In this assignment, you're required to clean up the two datasets. You'll be using these datasets in the later checkpoints of this module and hence cleaning them up here will help you save time when working with these datasets.\n",
        "\n",
        "The first dataset is a dialogue dataset called **Cornell Movie--Dialogs Corpus**. This corpus includes conversations between the characters of more than 600 movies.\n",
        "\n",
        "The second dataset is the **Twitter US Airline Sentiment dataset** from Kaggle. This dataset contains the tweets from travelers about some airlines in February 2015. This dataset is usually used in sentiment analysis but we'll use it for sentence generation later on.\n",
        "\n",
        "Since the memory requirements of the datasets are relatively large, we recommend you to use Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAzm2aoSjT5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Apply th data preprocessing techniques you learned here to Cornell Movie--Dialogs Corpus data. You'll be using this dataset when developing a chatbot in a later checkpoint. You should access the dataset from the Thinkful database using the following credentials:\n",
        "\n",
        " postgres_user = 'dsbc_student'\n",
        " postgres_pw = '7*.8G9QH21'\n",
        " postgres_host = '142.93.121.174'\n",
        " postgres_port = '5432'\n",
        " postgres_db = 'cornell_movie_dialogs'\n",
        "\n",
        " #The data is in the table called \"dialogs\".\n",
        "#Apply the data preprocessing techniques you learned here to Twitter US Airline Sentiment data. You'll be using this dataset when generating sentences in a later checkpoint. \n",
        "#You should access the dataset from the Thinkful database using the following credentials:\n",
        "\n",
        " postgres_user = 'dsbc_student'\n",
        " postgres_pw = '7*.8G9QH21'\n",
        " postgres_host = '142.93.121.174'\n",
        " postgres_port = '5432'\n",
        " postgres_db = 'twitter_sentiment'\n",
        "\n",
        " ##The data is in the table called \"twitter\".\n",
        "Note: When parsing the data using SpaCy, you may run into some memory issues even in Google Colaboratory. If you're having memory issues, try parsing your text as follows:\n",
        "\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "nlp.max_length = 20000000\n",
        "doc = nlp(the_dialogs_come_here)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqUi2X_jjvcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import linear_model\n",
        "import matplotlib.pyplot as plt\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_-BpwpskF8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " postgres_user = 'dsbc_student'\n",
        " postgres_pw = '7*.8G9QH21'\n",
        " postgres_host = '142.93.121.174'\n",
        " postgres_port = '5432'\n",
        " postgres_db = 'cornell_movie_dialogs'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w8Aa7G0kKve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
        "    postgres_user, postgres_pw, postgres_host, postgres_port, postgres_db))\n",
        "dialog0 = pd.read_sql_query('select * FROM dialogs',con=engine)\n",
        "\n",
        "# no need for an open connection, as we're only doing a single query\n",
        "engine.dispose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR0DDOJ7o7RH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dialog=dialog0.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3OzRdY4pAZy",
        "colab_type": "code",
        "outputId": "e1459f55-4c61-4aec-9b1a-91d2893db4eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "dialog.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>dialogs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Can we make this quick?  Roxanne Korrine and A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Not the hacking and gagging and spitting part....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Okay... then how 'bout we try out some French ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>You're asking me out.  That's so cute. What's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>Forget it.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>No, no, it's my fault -- we didn't have a prop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>Cameron.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>The thing is, Cameron -- I'm at the mercy of a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>Seems like she could get a date easy enough...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index                                            dialogs\n",
              "0      0  Can we make this quick?  Roxanne Korrine and A...\n",
              "1      1  Well, I thought we'd start with pronunciation,...\n",
              "2      2  Not the hacking and gagging and spitting part....\n",
              "3      3  Okay... then how 'bout we try out some French ...\n",
              "4      4  You're asking me out.  That's so cute. What's ...\n",
              "5      5                                         Forget it.\n",
              "6      6  No, no, it's my fault -- we didn't have a prop...\n",
              "7      7                                           Cameron.\n",
              "8      8  The thing is, Cameron -- I'm at the mercy of a...\n",
              "9      9     Seems like she could get a date easy enough..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KATuWBz4pSGU",
        "colab_type": "code",
        "outputId": "486bca5f-90c3-4e01-e460-f4ef39511118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dialog.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(304446, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1HueprrQPvP",
        "colab_type": "code",
        "outputId": "dafb1358-44e4-4d17-eed1-679e9a8e2dae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dialog['dialogs'][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyF52BOXQeR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dialog2=dialog['dialogs']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJb-QgUbQuj6",
        "colab_type": "code",
        "outputId": "6fd217a6-c790-4065-ed1e-f64e00472fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dialog2[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if26Sndet0j7",
        "colab_type": "code",
        "outputId": "73de9ca9-6222-44ef-8f0a-a93844cf6cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "example_sentence = dialog.loc[2]\n",
        "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here is an example: \n",
            "index                                                      2\n",
            "dialogs    Not the hacking and gagging and spitting part....\n",
            "Name: 2, dtype: object\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI03CqJ9uLN7",
        "colab_type": "code",
        "outputId": "1d12cab4-6e6b-46a6-ae36-0f377bea1e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_words = [token for token in example_sentence]\n",
        "unique_words = set([token for token in example_words])\n",
        "\n",
        "print((\"There are {} words in this sentence, and {} of them are\"\n",
        "       \" unique.\").format(len(example_words), len(unique_words)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 2 words in this sentence, and 2 of them are unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "engTLN_NveWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df['new_col'] = df['text'].apply(lambda x: nlp(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8hIM0KGFCo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern = \"[\\[].*?[\\]]\"\n",
        "\n",
        "alice = re.sub(pattern, \"\", alice)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHMNb4tEIWUM",
        "colab_type": "code",
        "outputId": "f07e1377-5962-4429-c897-a844e8c7a98c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(dialog)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "304446"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de1MUgLXKDJX",
        "colab_type": "text"
      },
      "source": [
        "**ADD ? TO PATTERN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXfZd5m0KByC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern = \"[\\[]\\?.*?[\\]]\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn-NVdgSI8we",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dialog.loc[0][1]=re.sub(pattern,\"\",dialog.loc[0][1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG667NHdKcC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=re.sub(pattern,\"\",dialog.loc[0][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggu5vL2lKh5O",
        "colab_type": "code",
        "outputId": "e9bd9ec9-d10c-4520-8b08-2a0c18e878c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsNNJ8I9MR-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern2='[\\?.\"(,*)!]'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-hE-CV0MawJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test2=re.sub(r'[,@\\'?\\.$%_]',\"\",dialog.loc[1][1],flags=re.I)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhSy2PMWlcJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern3=r'[-,@\\'?\\.$%_]'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p96lgq5fMkw7",
        "colab_type": "code",
        "outputId": "4719d61b-3d47-41a8-e83e-3ae08526b4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(test2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well I thought wed start with pronunciation if thats okay with you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjLQ28b0k3nP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test3=re.sub(r'[^a-zA-z0-9\\s]',\"\",dialog.loc[1][1],flags=re.I)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN1UUO4ilLdN",
        "colab_type": "code",
        "outputId": "206f9a15-a51f-4c1f-ae4e-0909e24b371a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(test3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well I thought wed start with pronunciation if thats okay with you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5l6JZDcbpCR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1_uza4XbpLM",
        "colab_type": "text"
      },
      "source": [
        "**XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX**\n",
        "\n",
        "**THIS IS THE TICKET**\n",
        "\n",
        "**SOMETHING I DID NOT KNOW**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkJVL1O7b1So",
        "colab_type": "text"
      },
      "source": [
        "**dialogs_doc = nlp(\" \".join(dialogs_df.dialogs))**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PzfJ8KictSO",
        "colab_type": "text"
      },
      "source": [
        "**XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdxjhHYgbpQ4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUH8JcrKXOR-",
        "colab_type": "text"
      },
      "source": [
        "**REPLACE CONTRACTIONS WITH FULL WORD-- DO THIS FIRST!**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZBAPb6UyhLH",
        "colab_type": "text"
      },
      "source": [
        "**THIS CONTRACTION UTILITY ASSUMES EVERYTHING IS lowercase**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9uHj_Q8Z1_E",
        "colab_type": "code",
        "outputId": "459dc402-6e6e-47c3-8026-e4dc4a6078bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "def _get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re\n",
        "\n",
        "contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "\n",
        "def replace_contractions(text):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "# Usage\n",
        "replace_contractions(\"this's a text with  isn't  didn't contraction\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is a text with  is not  did not contraction'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc3LktKTqw5U",
        "colab_type": "text"
      },
      "source": [
        "**NEW VERSION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBsFwXDHsdwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "what=(\" \".join(dialog.dialogs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjcgsZIbsjqp",
        "colab_type": "code",
        "outputId": "9e7397b0-2b63-4f62-ba06-e9b8b074d712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(what)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "romCsphwsout",
        "colab_type": "code",
        "outputId": "96ec41cc-4088-4d9f-8206-47603248b2fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "what[:400]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. Well, I thought we'd start with pronunciation, if that's okay with you. Not the hacking and gagging and spitting part.  Please. Okay... then how 'bout we try out some French cuisine.  Saturday?  Night? You're asking me out.  That's so cute. What's your name again? F\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmS9AiYCq3Qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dialog_test =replace_contractions(\" \".join(dialog.dialogs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh17mkm5pLai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "#necessary to avoid memory error of SpaCy\n",
        "nlp.max_length = 20000000\n",
        "\n",
        "dialogs_doc = nlp(\" \".join(dialog.dialogs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcohfAvzO4up",
        "colab_type": "code",
        "outputId": "598bce84-ecfe-4fcf-9090-3e02dfe1cf4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dialog_test[:400]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. Well, I thought we would start with pronunciation, if that is okay with you. Not the hacking and gagging and spitting part.  Please. Okay... then how 'bout we try out some French cuisine.  Saturday?  Night? You're asking me out.  That's so cute. What's your name aga\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR3Dvg8-YgmU",
        "colab_type": "text"
      },
      "source": [
        "**NEXT APPLY REGEX**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ip65jSJYz6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern3=r'[-,@\\'?\\.$%_]'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1Gfc0z5Ywfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dialog_test=[re.sub(pattern3,\"\",dialog.loc[x][1]) for x in range(len(dialog))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfpbaVT4ZHKl",
        "colab_type": "code",
        "outputId": "7ceea57e-008c-430f-bb1c-91d8a48a874a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "for row in dialog_test[:5]:\n",
        "  print(row)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Can we make this quick  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break up on the quad  Again\n",
            "Well I thought wed start with pronunciation if thats okay with you\n",
            "Not the hacking and gagging and spitting part  Please\n",
            "Okay then how bout we try out some French cuisine  Saturday  Night\n",
            "Youre asking me out  Thats so cute Whats your name again\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFGXIUvabuLW",
        "colab_type": "code",
        "outputId": "511460a9-fad7-484d-f077-37ef0b13ec66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dialog_test[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can we make this quick  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break up on the quad  Again'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY1okBrYb_Se",
        "colab_type": "code",
        "outputId": "6bbd4724-824c-479c-c2b0-f1973cbef274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(dialog_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dddK5kTZMcb",
        "colab_type": "text"
      },
      "source": [
        "**GET RID OF WHITESPACE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lgBXof-ZsuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dialog_test=[' '.join(dialog_test[x].split()) for x in range(len(dialog))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKWJbN69akK_",
        "colab_type": "code",
        "outputId": "71a8d8ec-fe9f-4fd4-cb44-feac0e0ae899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "for row in dialog_test[:5]:\n",
        "  print(row)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Can we make this quick Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break up on the quad Again\n",
            "Well I thought wed start with pronunciation if thats okay with you\n",
            "Not the hacking and gagging and spitting part Please\n",
            "Okay then how bout we try out some French cuisine Saturday Night\n",
            "Youre asking me out Thats so cute Whats your name again\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4Pg7GbigTch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBoLdtYJayOQ",
        "colab_type": "text"
      },
      "source": [
        "  **TOKENIZE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKL_AdH1av0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en',disable=['parser','ner'])\n",
        "\n",
        "# all the processing work is done below, so it may take a while\n",
        "alice_doc = nlp(alice)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8oZgMIXccy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dialog_test_doc=[nlp(dialog_test[x]) for x in range(len(dialog_test))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQGET0VdnbMN",
        "colab_type": "code",
        "outputId": "135649a7-939b-475f-f052-50fcc94ad7f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(dialog_test_doc[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hskoyHCF9CvJ",
        "colab_type": "code",
        "outputId": "5e60eb41-8ca6-43b9-c9ec-ec92872aab69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(dialog_test_doc[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbxUG_yRnwf-",
        "colab_type": "code",
        "outputId": "18e0179c-7f6f-481b-be6b-a31d8e718274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# let's explore the objects we've built.\n",
        "print(\"The dialog_test_doc  object is a {} object.\".format(type(dialog_test_doc)))\n",
        "print(\"It is {} tokens long\".format(len(dialog_test_doc)))\n",
        "print(\"The first three tokens are '{}'\".format(dialog_test_doc[:3]))\n",
        "print(\"The type of each token is {}\".format(type(dialog_test_doc[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dialog_test_doc  object is a <class 'list'> object.\n",
            "It is 304446 tokens long\n",
            "The first three tokens are '[Can we make this quick Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break up on the quad Again, Well I thought wed start with pronunciation if thats okay with you, Not the hacking and gagging and spitting part Please]'\n",
            "The type of each token is <class 'spacy.tokens.doc.Doc'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa3ATaEAo4JL",
        "colab_type": "code",
        "outputId": "9a85e38e-58d0-499d-e5ae-eeb1fff39d43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dialog_test_doc[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Can we make this quick Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break up on the quad Again"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBmPWXTSsu4e",
        "colab_type": "code",
        "outputId": "05322fdc-f959-428c-c10b-2bdc82f8ebe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dialog_test_doc[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Well I thought wed start with pronunciation if thats okay with you"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4vRYyftoQB0",
        "colab_type": "text"
      },
      "source": [
        "  **REMOVING STOPWORDS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI1SFiy0oYN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dialog_without_stopwords = [[token for token in dialog_test_doc[x] if not token.is_stop] for x in range(len(dialog_test_doc))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_vI_8XD6xuH",
        "colab_type": "code",
        "outputId": "d6b5d4dd-38f4-4ff4-d31c-96a2aae6552f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(dialog_without_stopwords)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "304446"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gYb5N3YsTp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tempo=[token for token in dialog_test_doc[0] if not token.is_stop]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZxLUrsksll4",
        "colab_type": "code",
        "outputId": "2d08582d-207b-4a66-d58c-15921c8413bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "tempo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[quick,\n",
              " Roxanne,\n",
              " Korrine,\n",
              " Andrew,\n",
              " Barrett,\n",
              " having,\n",
              " incredibly,\n",
              " horrendous,\n",
              " public,\n",
              " break,\n",
              " quad]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixw6LCyQreZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dialog_without_stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8b_XT5NvFJk",
        "colab_type": "text"
      },
      "source": [
        "**I NEED TO MAKE ONE LIST OUT OF DIALOG_WITHOUT_STOPWORDS**\n",
        "\n",
        "**THEN I CAN FEED LIST INTO WORD_FREQUENCIES()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V757fB9uviEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words_big_list=[]\n",
        "for x in range(len(dialog_without_stopwords)):\n",
        "   stop_words_big_list.extend(dialog_without_stopwords[x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vChOGUUdwzUt",
        "colab_type": "code",
        "outputId": "ac8849ea-0f71-471c-8da1-e60ad77b5da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(stop_words_big_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1458491"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGYbRHbYyIqE",
        "colab_type": "code",
        "outputId": "f926a7f4-adc1-4771-a10f-a87614bf6235",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "stop_words_big_list[:20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[quick,\n",
              " Roxanne,\n",
              " Korrine,\n",
              " Andrew,\n",
              " Barrett,\n",
              " having,\n",
              " incredibly,\n",
              " horrendous,\n",
              " public,\n",
              " break,\n",
              " quad,\n",
              " thought,\n",
              " d,\n",
              " start,\n",
              " pronunciation,\n",
              " s,\n",
              " okay,\n",
              " hacking,\n",
              " gagging,\n",
              " spitting]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XST7vcEdw8B4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dialog_word_freq=word_frequencies(stop_words_big_list).most_common(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4S9OWpw51o2",
        "colab_type": "code",
        "outputId": "ba622b19-fe47-4ed3-de5a-71496d57a991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "dialog_word_freq"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('nt', 55255),\n",
              " ('s', 32046),\n",
              " ('m', 22404),\n",
              " ('know', 21378),\n",
              " ('like', 13691),\n",
              " ('got', 12653),\n",
              " ('want', 10791),\n",
              " ('ve', 10667),\n",
              " ('think', 10397),\n",
              " ('going', 8762)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTg6Sg77pTR1",
        "colab_type": "code",
        "outputId": "4c0446e9-a4b2-4b96-f4da-0b17da46bc8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(dialog_without_stopwords)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "304446"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3nuGKrvpvu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utility function to calculate how frequently words appear in the text\n",
        "def word_frequencies(text):\n",
        "    \n",
        "    # build a list of words\n",
        "    # strip out punctuation\n",
        "    words = []\n",
        "    for token in text:\n",
        "        if not token.is_punct:\n",
        "            words.append(token.text)\n",
        "            \n",
        "    # build and return a Counter object containing word counts\n",
        "    return Counter(words)\n",
        "\n",
        "# instantiate our list of most common words.\n",
        "#alice_word_freq = word_frequencies(alice_without_stopwords).most_common(10)\n",
        "#print('\\nAlice:', alice_word_freq)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZnRmnyTrL03",
        "colab_type": "code",
        "outputId": "95f706cd-de5e-4358-b762-ea128b9b5fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "word_frequencies(dialog_without_stopwords[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'Andrew': 1,\n",
              "         'Barrett': 1,\n",
              "         'Korrine': 1,\n",
              "         'Roxanne': 1,\n",
              "         'break': 1,\n",
              "         'having': 1,\n",
              "         'horrendous': 1,\n",
              "         'incredibly': 1,\n",
              "         'public': 1,\n",
              "         'quad': 1,\n",
              "         'quick': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gemge2iyp99o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dialog_word_frequency=[word_frequencies(dialog_without_stopwords[x]) for x in range(len(dialog_test_doc))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiI8n9h0q7hO",
        "colab_type": "code",
        "outputId": "1cc0f1a9-a23e-43c3-fab4-a54a11744020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "dialog_word_frequency[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Counter({'Andrew': 1,\n",
              "          'Barrett': 1,\n",
              "          'Korrine': 1,\n",
              "          'Roxanne': 1,\n",
              "          'break': 1,\n",
              "          'having': 1,\n",
              "          'horrendous': 1,\n",
              "          'incredibly': 1,\n",
              "          'public': 1,\n",
              "          'quad': 1,\n",
              "          'quick': 1}),\n",
              " Counter({'d': 1,\n",
              "          'okay': 1,\n",
              "          'pronunciation': 1,\n",
              "          's': 1,\n",
              "          'start': 1,\n",
              "          'thought': 1}),\n",
              " Counter({'gagging': 1, 'hacking': 1, 'spitting': 1}),\n",
              " Counter({'French': 1,\n",
              "          'Night': 1,\n",
              "          'Okay': 1,\n",
              "          'Saturday': 1,\n",
              "          'bout': 1,\n",
              "          'cuisine': 1,\n",
              "          'try': 1}),\n",
              " Counter({'asking': 1, 'cute': 1, 's': 2})]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hgqre4Ya5MA",
        "colab_type": "text"
      },
      "source": [
        "**//////////////////////////////////////////////////////////**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh9usCsobkwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list(nlp(dialog.loc[0][1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fFtic_2b3Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_doc = nlp(dialog.loc[0][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIE8KF_rcThW",
        "colab_type": "code",
        "outputId": "09827cd6-0de7-4c7d-8ace-ddb175a6232f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_doc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k24xNTc2cb4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testy=list(test_doc.sents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JMeTjJvdhR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_sentence=testy[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oAqHAH9c77H",
        "colab_type": "code",
        "outputId": "67c4961b-d0e2-4370-e231-0e172837fcc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_words = [token for token in example_sentence if not token.is_punct]\n",
        "unique_words = set([token.text for token in example_words])\n",
        "\n",
        "print((\"There are {} words in this sentence, and {} of them are\"\n",
        "       \" unique.\").format(len(example_words), len(unique_words)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 words in this sentence, and 1 of them are unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff0-MC491dYp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXphQ9_x1dqH",
        "colab_type": "text"
      },
      "source": [
        "##**TWITTER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxGJUh8m1pwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "postgres_user = 'dsbc_student'\n",
        "postgres_pw = '7*.8G9QH21'\n",
        "postgres_host = '142.93.121.174'\n",
        "postgres_port = '5432'\n",
        "postgres_db = 'twitter_sentiment'\n",
        "\n",
        "##The data is in the table called \"twitter\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_BZzgvT2CRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
        "    postgres_user, postgres_pw, postgres_host, postgres_port, postgres_db))\n",
        "twitter0 = pd.read_sql_query('select * FROM twitter',con=engine)\n",
        "\n",
        "# no need for an open connection, as we're only doing a single query\n",
        "engine.dispose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3XGVk7V2Kgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twit=twitter0.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFlhDn-12XOc",
        "colab_type": "code",
        "outputId": "8ca19452-3c6c-499b-8b13-70f9aef346fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "twit.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP79TsRC2esw",
        "colab_type": "code",
        "outputId": "4849a733-3580-448e-d630-5d2c624456b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "source": [
        "twit.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>None</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>None</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>None</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>None</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>None</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>None</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>None</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>None</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>None</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>None</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>None</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>None</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>None</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>None</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index            tweet_id  ... tweet_location               user_timezone\n",
              "0      0  570306133677760513  ...           None  Eastern Time (US & Canada)\n",
              "1      1  570301130888122368  ...           None  Pacific Time (US & Canada)\n",
              "2      2  570301083672813571  ...      Lets Play  Central Time (US & Canada)\n",
              "3      3  570301031407624196  ...           None  Pacific Time (US & Canada)\n",
              "4      4  570300817074462722  ...           None  Pacific Time (US & Canada)\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kYRZOav3PcI",
        "colab_type": "code",
        "outputId": "dbb454e3-1ea4-458e-eb79-300df521be05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "list(twit)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['index',\n",
              " 'tweet_id',\n",
              " 'airline_sentiment',\n",
              " 'airline_sentiment_confidence',\n",
              " 'negativereason',\n",
              " 'negativereason_confidence',\n",
              " 'airline',\n",
              " 'airline_sentiment_gold',\n",
              " 'name',\n",
              " 'negativereason_gold',\n",
              " 'retweet_count',\n",
              " 'text',\n",
              " 'tweet_coord',\n",
              " 'tweet_created',\n",
              " 'tweet_location',\n",
              " 'user_timezone']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa0V3osq3HE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# below is necessary to avoid memory error of SpaCy\n",
        "nlp.max_length = 20000000\n",
        "\n",
        "# all the processing work is done below, so it may take a while\n",
        "twit_doc = nlp(\" \".join(twit.text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW2zCSJK3kIN",
        "colab_type": "code",
        "outputId": "3c8c595e-a60f-4a03-8cce-5a3369dbd47c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# let's explore the objects we've built.\n",
        "print(\"The twit_doc object is a {} object.\".format(type(twit_doc)))\n",
        "print(\"It is {} tokens long\".format(len(twit_doc)))\n",
        "print(\"The first three tokens are '{}'\".format(twit_doc[:3]))\n",
        "print(\"The type of each token is {}\".format(type(twit_doc[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The twit_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
            "It is 307328 tokens long\n",
            "The first three tokens are '@VirginAmerica What @dhepburn'\n",
            "The type of each token is <class 'spacy.tokens.token.Token'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHO_9vCl4Ezw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# removing the stopwords\n",
        "twit_docW = [token for token in twit_doc if not token.is_stop]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivO3ibEo4TDm",
        "colab_type": "code",
        "outputId": "b519cc4c-5230-40ef-c1ae-8a1faab89bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# let's explore the without stop words objects we've built.\n",
        "print(\"The twit_doc object is a {} object.\".format(type(twit_docW)))\n",
        "print(\"It is {} tokens long\".format(len(twit_docW)))\n",
        "print(\"The first three tokens are '{}'\".format(twit_docW[:3]))\n",
        "print(\"The type of each token is {}\".format(type(twit_docW[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The twit_doc object is a <class 'list'> object.\n",
            "It is 178303 tokens long\n",
            "The first three tokens are '[@VirginAmerica, @dhepburn, said]'\n",
            "The type of each token is <class 'spacy.tokens.token.Token'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNmEfz294qPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lemmatization\n",
        "lemmas = [token.lemma_ for token in twit_docW]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89W5Q0jY40Ed",
        "colab_type": "code",
        "outputId": "6d5caf59-5264-438a-cd87-0776e4568afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(lemmas)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbagyuBZ45Ru",
        "colab_type": "code",
        "outputId": "d2927025-f726-490b-fe04-c7e6ccf06c0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# let's explore the LEMMA objects we've built.\n",
        "print(\"The LEMMAS list  {} object.\".format(type(lemmas)))\n",
        "print(\"It is {} tokens long\".format(len(lemmas)))\n",
        "print(\"The first three tokens are '{}'\".format(lemmas[:3]))\n",
        "print(\"The type of each token is {}\".format(type(lemmas)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The LEMMAS list  <class 'list'> object.\n",
            "It is 178303 tokens long\n",
            "The first three tokens are '['@VirginAmerica', '@dhepburn', 'say']'\n",
            "The type of each token is <class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}